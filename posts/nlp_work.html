<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><title>自然语言处理课后题 | 奇幻空间</title><meta name="author" content="优雅的修勾"><meta name="copyright" content="优雅的修勾"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#f7f9fe"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="自然语言处理课后题"><meta name="application-name" content="自然语言处理课后题"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f7f9fe"><meta property="og:type" content="article"><meta property="og:title" content="自然语言处理课后题"><meta property="og:url" content="https://love.xiugou.top/posts/nlp_work.html"><meta property="og:site_name" content="奇幻空间"><meta property="og:description" content="下载项目资源： 官方参考答案 (PDF, 2.4MB)   第二章2.1 基于规则与基于机器学习的自然语言处理方法分别有哪些优缺点？ 基于规则的方法优点：  规则明确 轻量级 可解释性强  缺点：  依赖人工经验 泛化能力差 扩展性差   基于机器学习的方法优点：  自动特征提取 泛化能力强 端到端"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://love.xiugou.top/personal.jpg"><meta property="article:author" content="优雅的修勾"><meta property="article:tag"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://love.xiugou.top/personal.jpg"><meta name="description" content="下载项目资源： 官方参考答案 (PDF, 2.4MB)   第二章2.1 基于规则与基于机器学习的自然语言处理方法分别有哪些优缺点？ 基于规则的方法优点：  规则明确 轻量级 可解释性强  缺点：  依赖人工经验 泛化能力差 扩展性差   基于机器学习的方法优点：  自动特征提取 泛化能力强 端到端"><link rel="shortcut icon" href="/favicon64.ico"><link rel="canonical" href="https://love.xiugou.top/posts/nlp_work.html"><link rel="preconnect" href="//cdn.cbd.int"/><meta name="google-site-verification" content="xxx"/><meta name="baidu-site-verification" content="code-xxx"/><meta name="msvalidate.01" content="xxx"/><link rel="manifest" href="/manifest.json"/><meta name="msapplication-TileColor" content="#3b70fc"/><link rel="mask-icon" href="/img/128.png" color="#5bbad5"/><link rel="apple-touch-icon" sizes="180x180" href="/img/128.png"/><link rel="apple-touch-icon-precomposed" sizes="180x180" href="/img/128.png"/><link rel="icon" type="image/png" sizes="32x32" href="/img/32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/img/16.png"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  linkPageTop: undefined,
  peoplecanvas: {"enable":true,"img":"https://upload-bbs.miyoushe.com/upload/2024/07/27/125766904/ba62475f396df9de3316a08ed9e65d86_5680958632268053399..png"},
  postHeadAiDescription: {"enable":true,"gptName":"AnZhiYu","mode":"local","switchBtn":false,"btnLink":"https://afdian.net/item/886a79d4db6711eda42a52540025c377","randomNum":3,"basicWordCount":1000,"key":"xxxx","Referer":"https://xx.xx/"},
  diytitle: {"enable":true,"leaveTitle":"w(ﾟДﾟ)w 不要走！再看看嘛！","backTitle":"♪(^∇^*)欢迎肥来！"},
  LA51: undefined,
  greetingBox: undefined,
  twikooEnvId: 'https://love-twikoo.xiugou.top/',
  commentBarrageConfig:undefined,
  music_page_default: "nav_music",
  root: '/',
  preloader: {"source":3},
  friends_vue_info: undefined,
  navMusic: true,
  mainTone: undefined,
  authorStatus: {"skills":["🤖️ 数码科技爱好者","🔍 分享与热心帮助","🏠 智能家居小能手","🔨 设计开发一条龙","🤝 专修交互与设计","🏃 脚踏实地行动派","🧱 团队小组发动机","💢 壮汉人狠话不多"]},
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简","rightMenuMsgToTraditionalChinese":"转为繁体","rightMenuMsgToSimplifiedChinese":"转为简体"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":330},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    simplehomepage: true,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"copy":true,"copyrightEbable":false,"limitCount":50,"languages":{"author":"作者: 优雅的修勾","link":"链接: ","source":"来源: 奇幻空间","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","copySuccess":"复制成功，复制和转载请标注本文地址"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#425AEF","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  shortcutKey: undefined,
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  configTitle: '奇幻空间',
  title: '自然语言处理课后题',
  postAI: '',
  pageFillDescription: '第二章, 2.1 基于规则与基于机器学习的自然语言处理方法分别有哪些优缺点？, 2.2 如何在词的独热表示中引入词性、词义等特征？请举例说明。, 2.3 奇异值分解方法是如何反映词之间的高阶关系的？, 2.4 在使用式（2-18）计算困惑度时如果其中的某一项概率为0如何处理？, 2.5 若使用逆向最大匹配算法对句子研究生命的起源进行分词结果是什么？是, 2.6 2.2.2节介绍的子词切分算法是否可以用于中文？若能应用则与中文分词相比有哪些优缺点？, 2.7 是否可以使用序列标注方法解决句法分析（短语结构和依存两种）问题？若能使用则如何进行？, 2.8 使用何种评价方法评价一个中文分词系统？并请编程实现该评价方法。, 第三章, 3.1 使用NLTK工具下载简·奥斯汀所著的Emma小说原文并去掉其中的停用词。, 3.2 使用NLTK提供的WordNet计算两个词（不是词义）的相似度计算方法为两词各种词义之间的最大相似度。, 3.3 使用NLTK提供的SentiWordNet工具计算一个句子的情感倾向性计算方法为每个词所处词性下的每个词义情感倾向性之和。, 3.4 使用真实文本对比LTP与正向最大匹配分词的结果并人工分析哪些结果LTP正确正向最大匹配错误；哪些结果LTP错误正向最大匹配正确；以及哪些结果两个结果都错误。, 3.5 分析view、reshape、transpose和permute四种调整张量形状方法各自擅长处理的问题。, 3.6 安装PyTorch并实际对比使用和不使用GPU时三个大张量相乘时的效率。, 3.7 下载最新的Common Crawl数据并实现抽取中文、去重、繁简转换、数据清洗等功能。, 第四章, 4.1 试证明Sigmoid函数的导数为$y′=y（1−y）$。, 4.2 式（4-5）中如何解决zi过大导致e的zi次方数值溢出的问题？, 4.3 若去掉式 （4-11） 中的 ReLU 激活函数该多层感知器是否还能处理异或问题？为什么？, 4.4 在使用卷积神经网络时如何解决有用特征长度大于卷积核宽度的问题？, 4.5 在循环神经网络中各时刻共享权重的机制有何优缺点？, 4.6 在处理长距离依赖关系时原始的循环神经网络与长短时记忆网络（LSTM）在机制上有何本质的区别？, 4.7 在Transformer中使用绝对位置的词向量或编码有何缺点？针对该缺点有何解决方案？, 4.8 实际运行本章处理情感分类和词性标注问题的代码并对比各种模型的准确率然后尝试使用多种方法提高每种模型的准确率。, 第五章, 5.1 实际运行本章提供的不同词向量学习模型代码观察不同超参数的设置对于词向 量性能的影响。, 5.2 在基于负采样的Skip-gram模型中试分析不同上下文窗口大小对于词向量的影 响分别在情感分类以及词性标注任务上进行验证。, 5.3 下载预训练GloVe词向量利用t-SNE对其进行可视化分析。, 5.4 分别从词表示以及实际应用两个角度分析静态词向量的优缺点。并针对其缺点 思考并提出一种合理的解决方案。, 5.5 提出一种针对低频词的词向量学习改进方案。, 5.6 将预训练词向量用于目标任务时在什么情形下冻结词向量比精调词向量 更合理？在情感分类任务上进行验证。, 第六章, 6.1 分别从词表示和语义组合的角度阐述动态词向量的特点以及其相比于静态词向 量的优势。, 6.2 以英文中常用的多义词bank为例使用AllenNLP提供的ELMo模型抽取其在不 同句子中的词向量并使用t-SNE进行可视化分析。, 6.3 实现基于ELMo的词性标注并对比ELMo不同层的特征对于模型性能的影响。, 6.4 使用Transformer结构实现ELMo模型中的前向、后向语言模型并分别从语言模 型困惑度和下游任务性能两个方面与LSTM语言模型对比分析。, 6.5 为了训练中文的ELMo模型需要对模型结构做哪些调整？, 6.6 除了以特征形式应用于下游任务动态词向量还有哪些潜在的应用场景？, 第七章, 7.1 从模型的角度对比分析GPT和BERT各自的优缺点是什么？, 7.2 阐述BERT的输入表示中为什么要包含位置向量？如果没有位置向量将有何影响？, 7.3 阐述应用三种不同掩码策略（MLM、WWM和NM）的BERT在预训练阶段和下游任务 精调中的异同点。, 7.4 BERT中的MLM预训练任务采用了15%的掩码概率请阐述增大或减小掩码概率对预 训练语言模型效果可能产生的影响。, 7.5 以情感分类数据集SST-2为例通过实验论证特征提取和模型精调两种BERT的典 型应用方式对下游任务效果的影响。, 7.6 在抽取式阅读理解任务中篇章与问题的拼接顺序会对模型效果产生何种影响？ 请以具体的抽取式阅读理解任务CMRC 2018为例进行实验并给出相应的实验结论。, 第八章, 8.1 阐述自回归语言模型和自编码语言模型的优缺点。, 8.2 阐述词向量因式分解与跨层参数共享对ALBERT模型解码时间的影响。, 8.3 相比传统通过文本切分的方式处理长文本阐述长文本处理模型处理阅读理解和 命名实体识别任务的优势。, 8.4 仿照8.4.4节中的介绍尝试构造GPT-3在问答任务上的输入形式。, 8.5 仿照7.4.2节中的介绍在SST-2数据集上使用RoBERTa-base和ELECTRA-base模 型训练单句文本分类模型并对比两者的实验效果。, 8.6 在MNLI数据集上利用TextBrewer工具包实现12层BERT-base-cased模型蒸馏至3 层的BERT模型要求准确率不低于81%, 第九章, 9.1 阐述多语言预训练模型研究的主要意义与应用场景。, 9.2 使用HuggingFace提供的transformers库分别实现基于mBERT模型与XLM-R模型 的跨语言自然语言推理并在相应的基准数据集XNLI上进行实验。, 9.3 试分析多媒体融合的预训练模型目前存在哪些主要的挑战或瓶颈？结合最新的相 关文献说明。, 9.4 融合了知识库的预训练模型（如 ERNIE、KnowBERT）存在哪些潜在的缺点？, 9.5 除了实体、关系等结构化知识目前还有哪些知识是预训练模型缺少的或者难以 从文本中直接学习到的？, 9.6 在多任务或多语言学习的过程中考虑到不同任务（语言）的数据量、难度往往 不一致在训练时应当注意哪些问题以及采用哪些策略？结合 MT-DNN、ERNIE 2.0模 型以及第8章介绍的T5等模型进行分析。下载项目资源官方参考答案第二章基于规则与基于机器学习的自然语言处理方法分别有哪些优缺点基于规则的方法优点规则明确轻量级可解释性强缺点依赖人工经验泛化能力差扩展性差基于机器学习的方法优点自动特征提取泛化能力强端到端优化缺点数据需求高计算资源消耗大易过拟合如何在词的独热表示中引入词性词义等特征请举例说明词性以下为例其在句子下去中为动词而在下场中为名词通过引入词性特征可以将下在不同上下文中的词性作为额外特征从而区分其用法词义以漂亮和美丽为例它们在语义上属于同义词通过等工具可以提取它们的共同语义信息并将其作为额外特征加入独热表示中奇异值分解方法是如何反映词之间的高阶关系的奇异值分解通过降维技术将高维的词上下文共现矩阵转化为低维潜在语义空间从而反映词之间的潜在关联其核心思想是通过矩阵分解提取词与上下文的共现模式将高频共现的词映射到相似的低维向量从而在潜在空间中计算词或文档的相似度在使用式计算困惑度时如果其中的某一项概率为如何处理平滑技术调整模型参数若使用逆向最大匹配算法对句子研究生命的起源进行分词结果是什么是否可以说明逆向最大匹配算法要优于正向最大匹配算法结果为研究生命的起源逆向最大匹配算法表现出更优的分词效果因其能更有效地处理长词的错误切分从而提高分词的准确性因此逆向最大匹配算法在此例中优于正向最大匹配算法节介绍的子词切分算法是否可以用于中文若能应用则与中文分词相比有哪些优缺点子词切分算法在中文中具有显著优势尤其在处理未登录词和动态扩展词表方面但其切分粒度可能弱化语义关联且需权衡计算效率与模型性能特性子词切分传统中文分词优势动态扩展词表处理未登录词能力强兼容预训练模型依赖词典规则明确易于实现劣势切分粒度可能影响语义未登录词处理依赖上下文计算复杂度高无法处理新词词表规模大泛化能力弱是否可以使用序列标注方法解决句法分析短语结构和依存两种问题若能使用则如何进行任务类型适用性实现方式依存结构分析可行为每个词分配依存关系标签如使用序列标注模型预测短语结构分析有限通过短语标注与图结构生成结合先预测短语边界再构建树状结构使用何种评价方法评价一个中文分词系统并请编程实现该评价方法第三章使用工具下载简奥斯汀所著的小说原文并去掉其中的停用词使用提供的计算两个词不是词义的相似度计算方法为两词各种词义之间的最大相似度使用提供的工具计算一个句子的情感倾向性计算方法为每个词所处词性下的每个词义情感倾向性之和使用真实文本对比与正向最大匹配分词的结果并人工分析哪些结果正确正向最大匹配错误哪些结果错误正向最大匹配正确以及哪些结果两个结果都错误分析和四种调整张量形状方法各自擅长处理的问题安装并实际对比使用和不使用时三个大张量相乘时的效率下载最新的数据并实现抽取中文去重繁简转换数据清洗等功能第四章试证明函数的导数为式中如何解决过大导致的次方数值溢出的问题使用对数变换将的计算转换为对数形式当很大时因此此时可近似计算为这样避免了直接计算从而防止溢出若去掉式中的激活函数该多层感知器是否还能处理异或问题为什么该多层感知器将无法处理异或问题多层感知器的非线性能力多层感知器通过在中间层引入非线性激活函数如实现非线性映射若去掉中间层的输出将直接为线性组合即整个网络的输出仍为线性函数无法捕捉异或问题的非线性特性数值与结构限制的引入使得网络能够学习复杂的非线性关系若去掉网络结构退化为纯线性模型其输出仅依赖于权重和偏置的线性组合无法适应异或问题的非线性决策边界在使用卷积神经网络时如何解决有用特征长度大于卷积核宽度的问题多尺度卷积核设计池化操作在循环神经网络中各时刻共享权重的机制有何优缺点优点参数效率时间依赖建模结构一致性缺点梯度问题模式适应性限制在处理长距离依赖关系时原始的循环神经网络与长短时记忆网络在机制上有何本质的区别信息保留与遗忘机制原始遗忘门决定哪些信息从细胞状态中丢弃输入门控制新信息的写入输出门决定哪些信息输出到当前时刻的隐藏状态这种机制使能够选择性地保留或遗忘信息从而缓解梯度消失问题并增强对长距离依赖的建模能力梯度问题的解决原始由于信息传递的非线性叠加梯度在反向传播时容易消失或爆炸限制了模型对长距离依赖的建模能力通过门控机制的引入梯度在反向传播时可以更稳定地传递避免了梯度消失爆炸的问题从而更有效地学习长距离依赖结构复杂性原始结构简单仅包含一个隐含层信息传递依赖于权重矩阵的重复使用导致对长距离依赖的建模能力有限通过增加三个门控模块结构复杂度显著提高但这种复杂性恰恰是其解决长距离依赖问题的关键实际应用效果原始在处理长序列任务如语言建模时效果不佳因信息丢失严重通过门控机制能够更准确地捕捉序列中的长期依赖关系成为处理长距离依赖的主流选择综上通过引入门控机制解决了原始在信息保留与梯度传播方面的缺陷从而显著提升了对长距离依赖的建模能力在中使用绝对位置的词向量或编码有何缺点针对该缺点有何解决方案缺点无法区分不同块中的相同位置冗余信息解决方案引入相对位置编码结合多头自注意力实际运行本章处理情感分类和词性标注问题的代码并对比各种模型的准确率然后尝试使用多种方法提高每种模型的准确率第五章实际运行本章提供的不同词向量学习模型代码观察不同超参数的设置对于词向量性能的影响在基于负采样的模型中试分析不同上下文窗口大小对于词向量的影响分别在情感分类以及词性标注任务上进行验证下载预训练词向量利用对其进行可视化分析分别从词表示以及实际应用两个角度分析静态词向量的优缺点并针对其缺点思考并提出一种合理的解决方案一词表示角度优点语义关联性低维稠密表示全局统计信息缺点一词多义问题低频词表示不足忽略词序和局部结构二实际应用角度优点加速模型收敛即插即用资源消耗低缺点上下文不敏感领域适应性差组合语义局限提出一种针对低频词的词向量学习改进方案引入子词信息将预训练词向量用于目标任务时在什么情形下冻结词向量比精调词向量更合理在情感分类任务上进行验证适用于目标任务训练数据量较小在情感分类任务中训练数据较少预训练词向量质量高且领域匹配或追求训练效率时冻结词向量更合理第六章分别从词表示和语义组合的角度阐述动态词向量的特点以及其相比于静态词向量的优势词表示特点动态性层次性优势解决一词多义健壮性更强语义组合特点上下文感知的组合任务自适应优势更精细的语义建模克服静态组合缺陷以英文中常用的多义词为例使用提供的模型抽取其在不同句子中的词向量并使用进行可视化分析实现基于的词性标注并对比不同层的特征对于模型性能的影响使用结构实现模型中的前向后向语言模型并分别从语言模型困惑度和下游任务性能两个方面与语言模型对比分析为了训练中文的模型需要对模型结构做哪些调整输入表示层的调整层的调整输出向量的组合方式模型结构的扩展超参数的调整数据预处理的调整模型评估与优化除了以特征形式应用于下游任务动态词向量还有哪些潜在的应用场景词义消歧与近邻分析多语言与跨语言处理层次化特征提取与任务适配第七章从模型的角度对比分析和各自的优缺点是什么生成式预训练语言模型优点自回归结构生成能力训练效率高缺点缺乏双向建模依赖人工标记生成质量不稳定双向编码器表示优点双向建模多任务适应性可解释性强缺点计算资源消耗大生成能力有限预训练阶段依赖人工标记阐述的输入表示中为什么要包含位置向量如果没有位置向量将有何影响的输入表示中包含位置向量是为了解决原始自注意力模型无法处理序列位置信息的问题位置向量为每个词分配唯一的位置信息确保模型能够捕捉序列中的顺序依赖关系若缺少位置向量模型将无法区分不同位置的词导致相同词在不同位置的表示相同无法有效建模上下文顺序从而显著降低语义理解能力阐述应用三种不同掩码策略和的在预训练阶段和下游任务精调中的异同点预训练阶段的差异掩码单位与覆盖范围以子词为最小掩码单位掩码的词或字覆盖单个词或字的上下文与相同但掩码单位为整词或整字避免跨词字的掩码掩码多个子词或多个字覆盖更长的上下文片段影响和更关注局部语义则强化全局依赖可能提升对长距离关系的建模能力训练效率动态掩码通过实时生成掩码模式提升数据复用效率但原始采用静态掩码因掩码跨度大可能降低训练数据多样性需更多数据轮数补偿下游任务精调的共性输入统一性所有策略均依赖分词生成输入序列精调阶段无需修改代码直接使用相同输入表示任务适配性无论掩码策略如何模型在精调时均能通过上下文还原掩码词无需额外调整中的预训练任务采用了的掩码概率请阐述增大或减小掩码概率对预训练语言模型效果可能产生的影响增大掩码概率会增加模型对上下文依赖的建模难度迫使模型学习更丰富的语义信息但过高的掩码率可能导致模型无法有效捕捉局部语义甚至影响训练稳定性减小掩码概率会降低模型对上下文的依赖使模型更关注局部特征可能削弱其对全局语义的理解能力因此的掩码概率在中是一个平衡点既保证了语义建模的深度又避免了训练不稳定以情感分类数据集为例通过实验论证特征提取和模型精调两种的典型应用方式对下游任务效果的影响在抽取式阅读理解任务中篇章与问题的拼接顺序会对模型效果产生何种影响请以具体的抽取式阅读理解任务为例进行实验并给出相应的实验结论第八章阐述自回归语言模型和自编码语言模型的优缺点自回归语言模型优点生成能力突出结构简单小样本学习能力强缺点缺乏双向上下文生成质量受限自编码语言模型优点双向上下文建模任务泛化性缺点生成能力较弱训练复杂度高阐述词向量因式分解与跨层参数共享对模型解码时间的影响由于参数量减少词向量的计算和存储成本降低从而在解码阶段减少了对词向量的计算负担间接提升了解码速度在解码过程中共享的参数减少了重复计算进一步加速了推理过程相比传统通过文本切分的方式处理长文本阐述长文本处理模型处理阅读理解和命名实体识别任务的优势保留上下文信息传统方法通过固定长度切分文本导致块间信息割裂削弱长距离依赖建模能力如通过逐步扩展上下文窗口使模型在测试阶段能处理更长文本增强全局注意力长文本模型引入全局注意力或稀疏注意力机制使模型能关注更广泛的上下文提升对长距离依赖的捕捉能力减少信息丢失传统切分方法破坏词与序列标签的一一对应关系而长文本模型通过优化注意力机制更准确地建模实体上下文仿照节中的介绍尝试构造在问答任务上的输入形式仿照节中的介绍在数据集上使用和模型训练单句文本分类模型并对比两者的实验效果在数据集上利用工具包实现层模型蒸馏至层的模型要求准确率不低于第九章阐述多语言预训练模型研究的主要意义与应用场景主要意义促进跨语言处理降低多语言系统的开发成本提升模型泛化能力主要应用场景零样本迁移多语言问答与翻译跨语言信息检索多语言情感分析与命名实体识别跨语言文档理解使用提供的库分别实现基于模型与模型的跨语言自然语言推理并在相应的基准数据集上进行实验试分析多媒体融合的预训练模型目前存在哪些主要的挑战或瓶颈结合最新的相关文献说明多模态数据对齐与融合的复杂性数据获取成本高计算资源需求大泛化能力与跨任务迁移的局限性模型可解释性与可控性不足融合了知识库的预训练模型如存在哪些潜在的缺点知识获取与更新成本高知识融合机制复杂对知识表示的依赖性强模型可解释性与可控性受限数据稀疏性问题训练效率与资源消耗除了实体关系等结构化知识目前还有哪些知识是预训练模型缺少的或者难以从文本中直接学习到的常识与背景知识因果关系与逻辑推理领域特定知识隐含的语义关系时间与事件顺序在多任务或多语言学习的过程中考虑到不同任务语言的数据量难度往往不一致在训练时应当注意哪些问题以及采用哪些策略结合模型以及第章介绍的等模型进行分析分阶段训练与任务优先级调整通过分阶段训练解决数据量不一致的问题其先预训练共享编码层再利用多任务学习阶段结合不同任务的标注数据进行微调例如在训练初期仅使用少量任务数据逐步增加任务数量避免因数据稀缺导致的模型过拟合采用连续多任务学习策略动态分配任务在各阶段的训练次数例如在维持总迭代次数不变的前提下为任务分配不同的训练权重使资源更集中在数据量少或难度高的任务上动态任务权重与正则化模型通过任务提示技术统一多任务训练将不同任务转化为文本生成问题例如在训练时结合任务描述如回答问题或翻译句子作为输入前缀使模型无需为每个任务单独设计结构在损失函数中引入任务相关正则化项防止因任务难度差异导致的灾难性遗忘例如在精调阶段同时保留预训练任务的损失确保模型在下游任务中不丢失通用知识',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-06-04 15:40:52',
  postMainColor: '',
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#18171d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#f7f9fe')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/music.css"><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><img class="loading-img nolazyload" alt="加载头像" src="/personal.jpg"/><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.getElementById('loading-box').classList.add("loaded");
  },
  initLoading: () => {
    document.getElementById('loading-box').classList.remove("loaded")
  }
}
window.addEventListener('load',()=> { preloader.endLoading() })
setTimeout(function(){preloader.endLoading();},10000)

if (false) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.10/progress_bar/progress_bar.css"/><script async="async" src="https://cdn.cbd.int/pace-js@1.2.4/pace.min.js" data-pace-options="{ &quot;restartOnRequestAfter&quot;:false,&quot;eventLag&quot;:false}"></script><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><a id="site-name" href="/" accesskey="h"><div class="title">奇幻空间</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)">PAGE_NAME</a></div></div><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 隧道</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 友链</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/link/"><i class="anzhiyufont anzhiyu-icon-link faa-tada" style="font-size: 0.9em;"></i><span> 友人帐</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/fcircle/"><i class="anzhiyufont anzhiyu-icon-artstation faa-tada" style="font-size: 0.9em;"></i><span> 朋友圈</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><i class="anzhiyufont anzhiyu-icon-envelope faa-tada" style="font-size: 0.9em;"></i><span> 留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/music/"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> 音乐馆</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/bangumis/"><i class="anzhiyufont anzhiyu-icon-bilibili faa-tada" style="font-size: 0.9em;"></i><span> 追番页</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/album/"><i class="anzhiyufont anzhiyu-icon-images faa-tada" style="font-size: 0.9em;"></i><span> 相册集</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/air-conditioner/"><i class="anzhiyufont anzhiyu-icon-fan faa-tada" style="font-size: 0.9em;"></i><span> 小空调</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 关于</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/about/"><i class="anzhiyufont anzhiyu-icon-paper-plane faa-tada" style="font-size: 0.9em;"></i><span> 关于本人</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/essay/"><i class="anzhiyufont anzhiyu-icon-lightbulb faa-tada" style="font-size: 0.9em;"></i><span> 闲言碎语</span></a></li><li><a class="site-page child faa-parent animated-hover" href="javascript:toRandomPost()"><i class="anzhiyufont anzhiyu-icon-shoe-prints1 faa-tada" style="font-size: 0.9em;"></i><span> 随便逛逛</span></a></li></ul></div></div></div><div id="nav-right"><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机前往一个文章" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><div class="nav-button" id="search-button"><a class="site-page social-icon search" href="javascript:void(0);" title="搜索🔍" accesskey="s"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span> 搜索</span></a></div><input id="center-console" type="checkbox"/><label class="widget" for="center-console" title="中控台" onclick="anzhiyu.switchConsole();"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" target="_blank"><img class="post-qr-code-img" alt="微信" src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" target="_blank"><img class="post-qr-code-img" alt="支付宝" src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"><div class="console-card" id="card-newest-comments"><div class="card-content"><div class="author-content-item-tips">互动</div><span class="author-content-item-title"> 最新评论</span></div><div class="aside-list"><span>正在加载中...</span></div></div></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">音乐</div><span class="author-content-item-title">灵魂的碰撞💥</span></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>文章</span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>归档</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/06/"><span class="card-archive-list-date">六月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/05/"><span class="card-archive-list-date">五月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">3</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/02/"><span class="card-archive-list-date">二月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">2</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/01/"><span class="card-archive-list-date">一月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/12/"><span class="card-archive-list-date">十二月 2024</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2022/02/"><span class="card-archive-list-date">二月 2022</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">1</span><span>篇</span></div></a></li></ul></div><hr/></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="显示模式切换" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="边栏显示控制"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div><div class="console-btn-item" id="consoleMusic" onclick="anzhiyu.musicToggle()" title="音乐开关"><a class="music-switch"><i class="anzhiyufont anzhiyu-icon-music"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" title="切换"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original">原创</a><span class="article-meta tags"></span></div></div><h1 class="post-title" itemprop="name headline">自然语言处理课后题</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" itemprop="dateCreated datePublished" datetime="2025-06-03T14:04:23.200Z" title="发表于 2025-06-03 22:04:23">2025-06-03</time><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-history post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" itemprop="dateCreated datePublished" datetime="2025-06-04T07:40:52.000Z" title="更新于 2025-06-04 15:40:52">2025-06-04</time></span></div><div class="meta-secondline"><span class="post-meta-separator">       </span><span class="post-meta-position" title="作者IP属地为徐州"><i class="anzhiyufont anzhiyu-icon-location-dot"></i>徐州</span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section><div id="post-top-cover"><img class="nolazyload" id="post-top-bg" src=""></div></header><main id="blog-container"><div class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container" itemscope itemtype="https://love.xiugou.top/posts/nlp_work.html"><header><h1 id="CrawlerTitle" itemprop="name headline">自然语言处理课后题</h1><span itemprop="author" itemscope itemtype="http://schema.org/Person">优雅的修勾</span><time itemprop="dateCreated datePublished" datetime="2025-06-03T14:04:23.200Z" title="发表于 2025-06-03 22:04:23">2025-06-03</time><time itemprop="dateCreated datePublished" datetime="2025-06-04T07:40:52.000Z" title="更新于 2025-06-04 15:40:52">2025-06-04</time></header><p>下载项目资源：<br>
<a href="/downloads/nlpanswer.pdf" download class="download-link">官方参考答案 (PDF, 2.4MB)</a>
</p>

<h1 id="第二章"><a href="#第二章" class="headerlink" title="第二章"></a>第二章</h1><h2 id="2-1-基于规则与基于机器学习的自然语言处理方法分别有哪些优缺点？"><a href="#2-1-基于规则与基于机器学习的自然语言处理方法分别有哪些优缺点？" class="headerlink" title="2.1 基于规则与基于机器学习的自然语言处理方法分别有哪些优缺点？"></a>2.1 基于规则与基于机器学习的自然语言处理方法分别有哪些优缺点？</h2><ul>
<li><p><strong>基于规则的方法</strong><br>优点：</p>
<ul>
<li><strong>规则明确</strong></li>
<li><strong>轻量级</strong></li>
<li><strong>可解释性强</strong></li>
</ul>
<p>缺点：</p>
<ul>
<li><strong>依赖人工经验</strong></li>
<li><strong>泛化能力差</strong></li>
<li><strong>扩展性差</strong></li>
</ul>
</li>
<li><p><strong>基于机器学习的方法</strong><br>优点：</p>
<ul>
<li><strong>自动特征提取</strong></li>
<li><strong>泛化能力强</strong></li>
<li><strong>端到端优化</strong></li>
</ul>
<p>缺点：</p>
<ul>
<li><strong>数据需求高</strong></li>
<li><strong>计算资源消耗大</strong></li>
<li><strong>易过拟合</strong></li>
</ul>
</li>
</ul>
<h2 id="2-2-如何在词的独热表示中引入词性、词义等特征？请举例说明。"><a href="#2-2-如何在词的独热表示中引入词性、词义等特征？请举例说明。" class="headerlink" title="2.2 如何在词的独热表示中引入词性、词义等特征？请举例说明。"></a>2.2 如何在词的独热表示中引入词性、词义等特征？请举例说明。</h2><ul>
<li>词性：以“下”为例，其在句子“下去”中为动词，而在“下场”中为名词。通过引入词性特征，可以将“下”在不同上下文中的词性作为额外特征，从而区分其用法</li>
<li>词义：以“漂亮”和“美丽”为例，它们在语义上属于同义词。通过WordNet等工具，可以提取它们的共同语义信息，并将其作为额外特征加入独热表示中。</li>
</ul>
<h2 id="2-3-奇异值分解方法是如何反映词之间的高阶关系的？"><a href="#2-3-奇异值分解方法是如何反映词之间的高阶关系的？" class="headerlink" title="2.3 奇异值分解方法是如何反映词之间的高阶关系的？"></a>2.3 奇异值分解方法是如何反映词之间的高阶关系的？</h2><p>奇异值分解（SVD）通过降维技术将高维的“词—上下文”共现矩阵转化为低维潜在语义空间，从而反映词之间的潜在关联。其核心思想是通过矩阵分解提取词与上下文的共现模式，将高频共现的词映射到相似的低维向量，从而在潜在空间中计算词或文档的相似度。</p>
<h2 id="2-4-在使用式（2-18）计算困惑度时，如果其中的某一项概率为0，如何处理？"><a href="#2-4-在使用式（2-18）计算困惑度时，如果其中的某一项概率为0，如何处理？" class="headerlink" title="2.4 在使用式（2-18）计算困惑度时，如果其中的某一项概率为0，如何处理？"></a>2.4 在使用式（2-18）计算困惑度时，如果其中的某一项概率为0，如何处理？</h2><ul>
<li><strong>平滑技术</strong></li>
<li><strong>调整模型参数</strong></li>
</ul>
<h2 id="2-5-若使用逆向最大匹配算法对句子“研究生命的起源”进行分词，结果是什么？是"><a href="#2-5-若使用逆向最大匹配算法对句子“研究生命的起源”进行分词，结果是什么？是" class="headerlink" title="2.5 若使用逆向最大匹配算法对句子“研究生命的起源”进行分词，结果是什么？是"></a>2.5 若使用逆向最大匹配算法对句子“研究生命的起源”进行分词，结果是什么？是</h2><p>否可以说明逆向最大匹配算法要优于正向最大匹配算法？</p>
<ul>
<li>结果为 <strong>“研究 生命 的起源”</strong></li>
<li>逆向最大匹配算法表现出更优的分词效果，因其能更有效地处理长词的错误切分，从而提高分词的准确性。因此，<strong>逆向最大匹配算法在此例中优于正向最大匹配算法</strong>。</li>
</ul>
<h2 id="2-6-2-2-2节介绍的子词切分算法是否可以用于中文？若能应用，则与中文分词相比有哪些优缺点？"><a href="#2-6-2-2-2节介绍的子词切分算法是否可以用于中文？若能应用，则与中文分词相比有哪些优缺点？" class="headerlink" title="2.6 2.2.2节介绍的子词切分算法是否可以用于中文？若能应用，则与中文分词相比有哪些优缺点？"></a>2.6 2.2.2节介绍的子词切分算法是否可以用于中文？若能应用，则与中文分词相比有哪些优缺点？</h2><p>子词切分算法在中文中具有显著优势，尤其在处理未登录词和动态扩展词表方面。但其切分粒度可能弱化语义关联，且需权衡计算效率与模型性能。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>特性</strong></th>
<th><strong>子词切分</strong></th>
<th><strong>传统中文分词</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>优势</strong></td>
<td>动态扩展词表、处理未登录词能力强、兼容预训练模型</td>
<td>依赖词典、规则明确、易于实现</td>
</tr>
<tr>
<td><strong>劣势</strong></td>
<td>切分粒度可能影响语义、未登录词处理依赖上下文、计算复杂度高</td>
<td>无法处理新词、词表规模大、泛化能力弱</td>
</tr>
</tbody>
</table>
</div>
<h2 id="2-7-是否可以使用序列标注方法解决句法分析（短语结构和依存两种）问题？若能使用，则如何进行？"><a href="#2-7-是否可以使用序列标注方法解决句法分析（短语结构和依存两种）问题？若能使用，则如何进行？" class="headerlink" title="2.7 是否可以使用序列标注方法解决句法分析（短语结构和依存两种）问题？若能使用，则如何进行？"></a>2.7 是否可以使用序列标注方法解决句法分析（短语结构和依存两种）问题？若能使用，则如何进行？</h2><div class="table-container">
<table>
<thead>
<tr>
<th><strong>任务类型</strong></th>
<th><strong>适用性</strong></th>
<th><strong>实现方式</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>依存结构分析</strong></td>
<td>✅ 可行</td>
<td>为每个词分配依存关系标签（如 <code>nsubj</code>、<code>obj</code>），使用序列标注模型预测。</td>
</tr>
<tr>
<td><strong>短语结构分析</strong></td>
<td>⚠️ 有限</td>
<td>通过短语标注与图结构生成结合，先预测短语边界再构建树状结构。</td>
</tr>
</tbody>
</table>
</div>
<h2 id="2-8-使用何种评价方法评价一个中文分词系统？并请编程实现该评价方法。"><a href="#2-8-使用何种评价方法评价一个中文分词系统？并请编程实现该评价方法。" class="headerlink" title="2.8 使用何种评价方法评价一个中文分词系统？并请编程实现该评价方法。"></a><del>2.8 使用何种评价方法评价一个中文分词系统？并请编程实现该评价方法。</del></h2><h1 id="第三章"><a href="#第三章" class="headerlink" title="第三章"></a>第三章</h1><h2 id="3-1-使用NLTK工具下载简·奥斯汀所著的Emma小说原文，并去掉其中的停用词。"><a href="#3-1-使用NLTK工具下载简·奥斯汀所著的Emma小说原文，并去掉其中的停用词。" class="headerlink" title="3.1 使用NLTK工具下载简·奥斯汀所著的Emma小说原文，并去掉其中的停用词。"></a><del>3.1 使用NLTK工具下载简·奥斯汀所著的Emma小说原文，并去掉其中的停用词。</del></h2><h2 id="3-2-使用NLTK提供的WordNet计算两个词（不是词义）的相似度，计算方法为两词各种词义之间的最大相似度。"><a href="#3-2-使用NLTK提供的WordNet计算两个词（不是词义）的相似度，计算方法为两词各种词义之间的最大相似度。" class="headerlink" title="3.2 使用NLTK提供的WordNet计算两个词（不是词义）的相似度，计算方法为两词各种词义之间的最大相似度。"></a><del>3.2 使用NLTK提供的WordNet计算两个词（不是词义）的相似度，计算方法为两词各种词义之间的最大相似度。</del></h2><h2 id="3-3-使用NLTK提供的SentiWordNet工具计算一个句子的情感倾向性，计算方法为每个词所处词性下的每个词义情感倾向性之和。"><a href="#3-3-使用NLTK提供的SentiWordNet工具计算一个句子的情感倾向性，计算方法为每个词所处词性下的每个词义情感倾向性之和。" class="headerlink" title="3.3 使用NLTK提供的SentiWordNet工具计算一个句子的情感倾向性，计算方法为每个词所处词性下的每个词义情感倾向性之和。"></a><del>3.3 使用NLTK提供的SentiWordNet工具计算一个句子的情感倾向性，计算方法为每个词所处词性下的每个词义情感倾向性之和。</del></h2><h2 id="3-4-使用真实文本对比LTP与正向最大匹配分词的结果，并人工分析哪些结果LTP正确，正向最大匹配错误；哪些结果LTP错误，正向最大匹配正确；以及哪些结果两个结果都错误。"><a href="#3-4-使用真实文本对比LTP与正向最大匹配分词的结果，并人工分析哪些结果LTP正确，正向最大匹配错误；哪些结果LTP错误，正向最大匹配正确；以及哪些结果两个结果都错误。" class="headerlink" title="3.4 使用真实文本对比LTP与正向最大匹配分词的结果，并人工分析哪些结果LTP正确，正向最大匹配错误；哪些结果LTP错误，正向最大匹配正确；以及哪些结果两个结果都错误。"></a><del>3.4 使用真实文本对比LTP与正向最大匹配分词的结果，并人工分析哪些结果LTP正确，正向最大匹配错误；哪些结果LTP错误，正向最大匹配正确；以及哪些结果两个结果都错误。</del></h2><h2 id="3-5-分析view、reshape、transpose和permute四种调整张量形状方法各自擅长处理的问题。"><a href="#3-5-分析view、reshape、transpose和permute四种调整张量形状方法各自擅长处理的问题。" class="headerlink" title="3.5 分析view、reshape、transpose和permute四种调整张量形状方法各自擅长处理的问题。"></a><del>3.5 分析view、reshape、transpose和permute四种调整张量形状方法各自擅长处理的问题。</del></h2><h2 id="3-6-安装PyTorch并实际对比使用和不使用GPU时，三个大张量相乘时的效率。"><a href="#3-6-安装PyTorch并实际对比使用和不使用GPU时，三个大张量相乘时的效率。" class="headerlink" title="3.6 安装PyTorch并实际对比使用和不使用GPU时，三个大张量相乘时的效率。"></a><del>3.6 安装PyTorch并实际对比使用和不使用GPU时，三个大张量相乘时的效率。</del></h2><h2 id="3-7-下载最新的Common-Crawl数据，并实现抽取中文、去重、繁简转换、数据清洗等功能。"><a href="#3-7-下载最新的Common-Crawl数据，并实现抽取中文、去重、繁简转换、数据清洗等功能。" class="headerlink" title="3.7 下载最新的Common Crawl数据，并实现抽取中文、去重、繁简转换、数据清洗等功能。"></a><del>3.7 下载最新的Common Crawl数据，并实现抽取中文、去重、繁简转换、数据清洗等功能。</del></h2><h1 id="第四章"><a href="#第四章" class="headerlink" title="第四章"></a>第四章</h1><h2 id="4-1-试证明Sigmoid函数的导数为-y′-y（1−y）-。"><a href="#4-1-试证明Sigmoid函数的导数为-y′-y（1−y）-。" class="headerlink" title="4.1 试证明Sigmoid函数的导数为$y′=y（1−y）$。"></a>4.1 试证明Sigmoid函数的导数为$y′=y（1−y）$。</h2><h2 id="4-2-式（4-5）中，如何解决zi过大，导致e的zi次方数值溢出的问题？"><a href="#4-2-式（4-5）中，如何解决zi过大，导致e的zi次方数值溢出的问题？" class="headerlink" title="4.2 式（4-5）中，如何解决zi过大，导致e的zi次方数值溢出的问题？"></a>4.2 式（4-5）中，如何解决zi过大，导致e的zi次方数值溢出的问题？</h2><p><strong>使用对数变换</strong></p>
<p>将$y=\frac{1}{1+e^{-z} }$​ 的计算转换为对数形式。当 z 很大时，$e^{-z}≈0$，因此$ y≈1$。此时，可近似计算为：$y=1-\frac{1}{1+e^{z}}$，这样避免了直接计算 $e^{z}$，从而防止溢出</p>
<h2 id="4-3-若去掉式-（4-11）-中的-ReLU-激活函数，该多层感知器是否还能处理异或问题？为什么？"><a href="#4-3-若去掉式-（4-11）-中的-ReLU-激活函数，该多层感知器是否还能处理异或问题？为什么？" class="headerlink" title="4.3 若去掉式 （4-11） 中的 ReLU 激活函数，该多层感知器是否还能处理异或问题？为什么？"></a>4.3 若去掉式 （4-11） 中的 ReLU 激活函数，该多层感知器是否还能处理异或问题？为什么？</h2><ul>
<li>该多层感知器将无法处理异或问题</li>
<li><strong>多层感知器的非线性能力：</strong>多层感知器（MLP）通过在中间层引入非线性激活函数（如ReLU）实现非线性映射。若去掉ReLU，中间层的输出将直接为线性组合（即 $h=z$），整个网络的输出仍为线性函数，无法捕捉异或问题的非线性特性。</li>
<li><strong>数值与结构限制：</strong>ReLU的引入使得网络能够学习复杂的非线性关系。若去掉ReLU，网络结构退化为纯线性模型，其输出仅依赖于权重和偏置的线性组合，无法适应异或问题的非线性决策边界</li>
</ul>
<h2 id="4-4-在使用卷积神经网络时，如何解决有用特征长度大于卷积核宽度的问题？"><a href="#4-4-在使用卷积神经网络时，如何解决有用特征长度大于卷积核宽度的问题？" class="headerlink" title="4.4 在使用卷积神经网络时，如何解决有用特征长度大于卷积核宽度的问题？"></a>4.4 在使用卷积神经网络时，如何解决有用特征长度大于卷积核宽度的问题？</h2><ul>
<li><strong>多尺度卷积核设计</strong></li>
<li><strong>池化操作</strong></li>
</ul>
<h2 id="4-5-在循环神经网络中，各时刻共享权重的机制有何优缺点？"><a href="#4-5-在循环神经网络中，各时刻共享权重的机制有何优缺点？" class="headerlink" title="4.5 在循环神经网络中，各时刻共享权重的机制有何优缺点？"></a>4.5 在循环神经网络中，各时刻共享权重的机制有何优缺点？</h2><p><strong>优点</strong></p>
<ul>
<li><strong>参数效率</strong></li>
<li><strong>时间依赖建模</strong></li>
<li><strong>结构一致性</strong></li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li><strong>梯度问题</strong></li>
<li><strong>模式适应性限制</strong></li>
</ul>
<h2 id="4-6-在处理长距离依赖关系时，原始的循环神经网络与长短时记忆网络（LSTM）在机制上有何本质的区别？"><a href="#4-6-在处理长距离依赖关系时，原始的循环神经网络与长短时记忆网络（LSTM）在机制上有何本质的区别？" class="headerlink" title="4.6 在处理长距离依赖关系时，原始的循环神经网络与长短时记忆网络（LSTM）在机制上有何本质的区别？"></a>4.6 在处理长距离依赖关系时，原始的循环神经网络与长短时记忆网络（LSTM）在机制上有何本质的区别？</h2><ul>
<li><p><strong>信息保留与遗忘机制</strong></p>
<ul>
<li><strong>原始RNN</strong></li>
<li><strong>LSTM</strong></li>
</ul>
</li>
<li><p><strong>遗忘门</strong>（ft​）决定哪些信息从细胞状态中丢弃。</p>
</li>
<li><strong>输入门</strong>（it​）控制新信息的写入。</li>
<li><p><strong>输出门</strong>（ot​）决定哪些信息输出到当前时刻的隐藏状态。<br>这种机制使LSTM能够选择性地保留或遗忘信息，从而缓解梯度消失问题并增强对长距离依赖的建模能力。</p>
</li>
<li><p><strong>梯度问题的解决</strong></p>
<ul>
<li><strong>原始RNN</strong>：由于信息传递的非线性叠加，梯度在反向传播时容易消失或爆炸，限制了模型对长距离依赖的建模能力。</li>
<li><strong>LSTM</strong>：通过门控机制的引入，梯度在反向传播时可以更稳定地传递，避免了梯度消失/爆炸的问题，从而更有效地学习长距离依赖。</li>
</ul>
</li>
<li><p><strong>结构复杂性</strong></p>
<ul>
<li><strong>原始RNN</strong>：结构简单，仅包含一个隐含层，信息传递依赖于权重矩阵的重复使用，导致对长距离依赖的建模能力有限。</li>
<li><strong>LSTM</strong>：通过增加三个门控模块，结构复杂度显著提高，但这种复杂性恰恰是其解决长距离依赖问题的关键。</li>
</ul>
</li>
<li><p><strong>实际应用效果</strong></p>
<ul>
<li><strong>原始RNN</strong>：在处理长序列任务（如语言建模）时效果不佳，因信息丢失严重。</li>
<li><strong>LSTM</strong>：通过门控机制，能够更准确地捕捉序列中的长期依赖关系，成为处理长距离依赖的主流选择。</li>
</ul>
</li>
</ul>
<p>综上，LSTM通过引入门控机制，解决了原始RNN在信息保留与梯度传播方面的缺陷，从而显著提升了对长距离依赖的建模能力。</p>
<h2 id="4-7-在Transformer中，使用绝对位置的词向量或编码有何缺点？针对该缺点有何解决方案？"><a href="#4-7-在Transformer中，使用绝对位置的词向量或编码有何缺点？针对该缺点有何解决方案？" class="headerlink" title="4.7 在Transformer中，使用绝对位置的词向量或编码有何缺点？针对该缺点有何解决方案？"></a>4.7 在Transformer中，使用绝对位置的词向量或编码有何缺点？针对该缺点有何解决方案？</h2><p>缺点：</p>
<ul>
<li><strong>无法区分不同块中的相同位置</strong></li>
<li><strong>冗余信息</strong><br>解决方案：</li>
<li><strong>引入相对位置编码</strong></li>
<li><strong>结合多头自注意力</strong></li>
</ul>
<h2 id="4-8-实际运行本章处理情感分类和词性标注问题的代码，并对比各种模型的准确率，然后尝试使用多种方法提高每种模型的准确率。"><a href="#4-8-实际运行本章处理情感分类和词性标注问题的代码，并对比各种模型的准确率，然后尝试使用多种方法提高每种模型的准确率。" class="headerlink" title="4.8 实际运行本章处理情感分类和词性标注问题的代码，并对比各种模型的准确率，然后尝试使用多种方法提高每种模型的准确率。"></a><del>4.8 实际运行本章处理情感分类和词性标注问题的代码，并对比各种模型的准确率，然后尝试使用多种方法提高每种模型的准确率。</del></h2><h1 id="第五章"><a href="#第五章" class="headerlink" title="第五章"></a>第五章</h1><h2 id="5-1-实际运行本章提供的不同词向量学习模型代码，观察不同超参数的设置对于词向-量性能的影响。"><a href="#5-1-实际运行本章提供的不同词向量学习模型代码，观察不同超参数的设置对于词向-量性能的影响。" class="headerlink" title="5.1 实际运行本章提供的不同词向量学习模型代码，观察不同超参数的设置对于词向 量性能的影响。"></a><del>5.1 实际运行本章提供的不同词向量学习模型代码，观察不同超参数的设置对于词向 量性能的影响。</del></h2><h2 id="5-2-在基于负采样的Skip-gram模型中，试分析不同上下文窗口大小对于词向量的影-响，分别在情感分类以及词性标注任务上进行验证。"><a href="#5-2-在基于负采样的Skip-gram模型中，试分析不同上下文窗口大小对于词向量的影-响，分别在情感分类以及词性标注任务上进行验证。" class="headerlink" title="5.2 在基于负采样的Skip-gram模型中，试分析不同上下文窗口大小对于词向量的影 响，分别在情感分类以及词性标注任务上进行验证。"></a><del>5.2 在基于负采样的Skip-gram模型中，试分析不同上下文窗口大小对于词向量的影 响，分别在情感分类以及词性标注任务上进行验证。</del></h2><h2 id="5-3-下载预训练GloVe词向量，利用t-SNE对其进行可视化分析。"><a href="#5-3-下载预训练GloVe词向量，利用t-SNE对其进行可视化分析。" class="headerlink" title="5.3 下载预训练GloVe词向量，利用t-SNE对其进行可视化分析。"></a><del>5.3 下载预训练GloVe词向量，利用t-SNE对其进行可视化分析。</del></h2><h2 id="5-4-分别从词表示以及实际应用两个角度分析静态词向量的优缺点。并针对其缺点，-思考并提出一种合理的解决方案。"><a href="#5-4-分别从词表示以及实际应用两个角度分析静态词向量的优缺点。并针对其缺点，-思考并提出一种合理的解决方案。" class="headerlink" title="5.4 分别从词表示以及实际应用两个角度分析静态词向量的优缺点。并针对其缺点， 思考并提出一种合理的解决方案。"></a>5.4 分别从词表示以及实际应用两个角度分析静态词向量的优缺点。并针对其缺点， 思考并提出一种合理的解决方案。</h2><p><strong>一、词表示角度</strong></p>
<p><strong>优点：</strong></p>
<ul>
<li><strong>语义关联性</strong></li>
<li><strong>低维稠密表示</strong></li>
<li><strong>全局统计信息</strong></li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li><strong>一词多义问题</strong></li>
<li><strong>低频词表示不足</strong></li>
<li><strong>忽略词序和局部结构</strong></li>
</ul>
<p><strong>二、实际应用角度</strong></p>
<p><strong>优点：</strong></p>
<ul>
<li><strong>加速模型收敛</strong></li>
<li><strong>即插即用</strong></li>
<li><strong>资源消耗低</strong></li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li><strong>上下文不敏感</strong></li>
<li><strong>领域适应性差</strong></li>
<li><strong>组合语义局限</strong></li>
</ul>
<h2 id="5-5-提出一种针对低频词的词向量学习改进方案。"><a href="#5-5-提出一种针对低频词的词向量学习改进方案。" class="headerlink" title="5.5 提出一种针对低频词的词向量学习改进方案。"></a>5.5 提出一种针对低频词的词向量学习改进方案。</h2><p><strong>引入子词信息</strong></p>
<h2 id="5-6-将预训练词向量用于目标任务时，在什么情形下，“冻结”词向量比精调词向量-更合理？在情感分类任务上进行验证。"><a href="#5-6-将预训练词向量用于目标任务时，在什么情形下，“冻结”词向量比精调词向量-更合理？在情感分类任务上进行验证。" class="headerlink" title="5.6 将预训练词向量用于目标任务时，在什么情形下，“冻结”词向量比精调词向量 更合理？在情感分类任务上进行验证。"></a>5.6 将预训练词向量用于目标任务时，在什么情形下，“冻结”词向量比精调词向量 更合理？在情感分类任务上进行验证。</h2><p><strong>适用于目标任务训练数据量较小</strong></p>
<p>在情感分类任务中，<strong>训练数据较少</strong>、<strong>预训练词向量质量高且领域匹配</strong>、或<strong>追求训练效率</strong>时，冻结词向量更合理。</p>
<h1 id="第六章"><a href="#第六章" class="headerlink" title="第六章"></a>第六章</h1><h2 id="6-1-分别从词表示和语义组合的角度阐述动态词向量的特点，以及其相比于静态词向-量的优势。"><a href="#6-1-分别从词表示和语义组合的角度阐述动态词向量的特点，以及其相比于静态词向-量的优势。" class="headerlink" title="6.1 分别从词表示和语义组合的角度阐述动态词向量的特点，以及其相比于静态词向 量的优势。"></a>6.1 分别从词表示和语义组合的角度阐述动态词向量的特点，以及其相比于静态词向 量的优势。</h2><p><strong>词表示特点：</strong></p>
<ul>
<li><strong>动态性</strong></li>
<li><strong>层次性</strong></li>
</ul>
<p><strong>优势：</strong></p>
<ul>
<li><strong>解决一词多义</strong></li>
<li><p><strong>健壮性更强</strong></p>
<p><strong>语义组合特点：</strong></p>
</li>
<li><strong>上下文感知的组合</strong></li>
<li><strong>任务自适应</strong></li>
</ul>
<p><strong>优势：</strong></p>
<ul>
<li><strong>更精细的语义建模</strong></li>
<li><strong>克服静态组合缺陷</strong></li>
</ul>
<h2 id="6-2-以英文中常用的多义词“bank”为例，使用AllenNLP提供的ELMo模型抽取其在不-同句子中的词向量，并使用t-SNE进行可视化分析。"><a href="#6-2-以英文中常用的多义词“bank”为例，使用AllenNLP提供的ELMo模型抽取其在不-同句子中的词向量，并使用t-SNE进行可视化分析。" class="headerlink" title="6.2 以英文中常用的多义词“bank”为例，使用AllenNLP提供的ELMo模型抽取其在不 同句子中的词向量，并使用t-SNE进行可视化分析。"></a><del>6.2 以英文中常用的多义词“bank”为例，使用AllenNLP提供的ELMo模型抽取其在不 同句子中的词向量，并使用t-SNE进行可视化分析。</del></h2><h2 id="6-3-实现基于ELMo的词性标注，并对比ELMo不同层的特征对于模型性能的影响。"><a href="#6-3-实现基于ELMo的词性标注，并对比ELMo不同层的特征对于模型性能的影响。" class="headerlink" title="6.3 实现基于ELMo的词性标注，并对比ELMo不同层的特征对于模型性能的影响。"></a><del>6.3 实现基于ELMo的词性标注，并对比ELMo不同层的特征对于模型性能的影响。</del></h2><h2 id="6-4-使用Transformer结构实现ELMo模型中的前向、后向语言模型，并分别从语言模-型困惑度和下游任务性能两个方面与LSTM语言模型对比分析。"><a href="#6-4-使用Transformer结构实现ELMo模型中的前向、后向语言模型，并分别从语言模-型困惑度和下游任务性能两个方面与LSTM语言模型对比分析。" class="headerlink" title="6.4 使用Transformer结构实现ELMo模型中的前向、后向语言模型，并分别从语言模 型困惑度和下游任务性能两个方面与LSTM语言模型对比分析。"></a><del>6.4 使用Transformer结构实现ELMo模型中的前向、后向语言模型，并分别从语言模 型困惑度和下游任务性能两个方面与LSTM语言模型对比分析。</del></h2><h2 id="6-5-为了训练中文的ELMo模型，需要对模型结构做哪些调整？"><a href="#6-5-为了训练中文的ELMo模型，需要对模型结构做哪些调整？" class="headerlink" title="6.5 为了训练中文的ELMo模型，需要对模型结构做哪些调整？"></a>6.5 为了训练中文的ELMo模型，需要对模型结构做哪些调整？</h2><ul>
<li><strong>输入表示层的调整</strong></li>
<li><strong>LSTM层的调整</strong></li>
<li><strong>输出向量的组合方式</strong></li>
<li><strong>模型结构的扩展</strong></li>
<li><strong>超参数的调整</strong></li>
<li><strong>数据预处理的调整</strong></li>
<li><strong>模型评估与优化</strong></li>
</ul>
<h2 id="6-6-除了以特征形式应用于下游任务，动态词向量还有哪些潜在的应用场景？"><a href="#6-6-除了以特征形式应用于下游任务，动态词向量还有哪些潜在的应用场景？" class="headerlink" title="6.6 除了以特征形式应用于下游任务，动态词向量还有哪些潜在的应用场景？"></a>6.6 除了以特征形式应用于下游任务，动态词向量还有哪些潜在的应用场景？</h2><ul>
<li><strong>词义消歧与近邻分析</strong></li>
<li><strong>多语言与跨语言处理</strong></li>
<li><strong>层次化特征提取与任务适配</strong></li>
</ul>
<h1 id="第七章"><a href="#第七章" class="headerlink" title="第七章"></a>第七章</h1><h2 id="7-1-从模型的角度对比分析GPT和BERT各自的优缺点是什么？"><a href="#7-1-从模型的角度对比分析GPT和BERT各自的优缺点是什么？" class="headerlink" title="7.1 从模型的角度对比分析GPT和BERT各自的优缺点是什么？"></a>7.1 从模型的角度对比分析GPT和BERT各自的优缺点是什么？</h2><p><strong>GPT（生成式预训练语言模型）</strong></p>
<p><strong>优点：</strong></p>
<ul>
<li><strong>自回归结构</strong></li>
<li><strong>生成能力</strong></li>
<li><strong>训练效率高</strong></li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li><strong>缺乏双向建模</strong></li>
<li><strong>依赖人工标记</strong></li>
<li><strong>生成质量不稳定</strong></li>
</ul>
<p>BERT（双向编码器表示）</p>
<p><strong>优点：</strong></p>
<ul>
<li><strong>双向建模</strong></li>
<li><strong>多任务适应性</strong></li>
<li><strong>可解释性强</strong></li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li><strong>计算资源消耗大</strong></li>
<li><strong>生成能力有限</strong></li>
<li><strong>预训练阶段依赖人工标记</strong></li>
</ul>
<h2 id="7-2-阐述BERT的输入表示中为什么要包含位置向量？如果没有位置向量将有何影响？"><a href="#7-2-阐述BERT的输入表示中为什么要包含位置向量？如果没有位置向量将有何影响？" class="headerlink" title="7.2 阐述BERT的输入表示中为什么要包含位置向量？如果没有位置向量将有何影响？"></a>7.2 阐述BERT的输入表示中为什么要包含位置向量？如果没有位置向量将有何影响？</h2><p>BERT的输入表示中包含位置向量是为了解决原始自注意力模型无法处理序列位置信息的问题。位置向量为每个词分配唯一的位置信息，确保模型能够捕捉序列中的顺序依赖关系。若缺少位置向量，模型将无法区分不同位置的词，导致相同词在不同位置的表示相同，无法有效建模上下文顺序，从而显著降低语义理解能力。</p>
<h2 id="7-3-阐述应用三种不同掩码策略（MLM、WWM和NM）的BERT，在预训练阶段和下游任务-精调中的异同点。"><a href="#7-3-阐述应用三种不同掩码策略（MLM、WWM和NM）的BERT，在预训练阶段和下游任务-精调中的异同点。" class="headerlink" title="7.3 阐述应用三种不同掩码策略（MLM、WWM和NM）的BERT，在预训练阶段和下游任务 精调中的异同点。"></a>7.3 阐述应用三种不同掩码策略（MLM、WWM和NM）的BERT，在预训练阶段和下游任务 精调中的异同点。</h2><p> <strong>预训练阶段的差异</strong></p>
<p> <strong>掩码单位与覆盖范围</strong></p>
<ul>
<li><strong>MLM</strong>：以WordPiece子词为最小掩码单位，掩码15%的词或字，覆盖单个词或字的上下文。</li>
<li><strong>WWM</strong>：与MLM相同，但掩码单位为整词或整字，避免跨词/字的掩码。</li>
<li><strong>NM</strong>：掩码多个WordPiece子词或多个字，覆盖更长的上下文片段。</li>
<li><p><strong>影响</strong>：MLM和WWM更关注局部语义，NM则强化全局依赖，可能提升对长距离关系的建模能力。</p>
<p><strong>训练效率</strong></p>
</li>
<li><p><strong>动态掩码：</strong>通过实时生成掩码模式，提升数据复用效率，但原始BERT采用静态掩码。</p>
</li>
<li><strong>NM：</strong>因掩码跨度大，可能降低训练数据多样性，需更多数据轮数补偿。</li>
</ul>
<p><strong>下游任务精调的共性</strong></p>
<ul>
<li><strong>输入统一性</strong>：所有策略均依赖WordPiece分词生成输入序列，精调阶段无需修改代码，直接使用相同输入表示。</li>
<li><strong>任务适配性</strong>：无论掩码策略如何，模型在精调时均能通过上下文还原掩码词，无需额外调整。</li>
</ul>
<h2 id="7-4-BERT中的MLM预训练任务采用了15-的掩码概率，请阐述增大或减小掩码概率对预-训练语言模型效果可能产生的影响。"><a href="#7-4-BERT中的MLM预训练任务采用了15-的掩码概率，请阐述增大或减小掩码概率对预-训练语言模型效果可能产生的影响。" class="headerlink" title="7.4 BERT中的MLM预训练任务采用了15%的掩码概率，请阐述增大或减小掩码概率对预 训练语言模型效果可能产生的影响。"></a>7.4 BERT中的MLM预训练任务采用了15%的掩码概率，请阐述增大或减小掩码概率对预 训练语言模型效果可能产生的影响。</h2><ul>
<li><p><strong>增大掩码概率</strong>：<br>会增加模型对上下文依赖的建模难度，迫使模型学习更丰富的语义信息。但过高的掩码率可能导致模型无法有效捕捉局部语义，甚至影响训练稳定性。</p>
</li>
<li><p><strong>减小掩码概率</strong>：<br>会降低模型对上下文的依赖，使模型更关注局部特征，可能削弱其对全局语义的理解能力。</p>
</li>
</ul>
<p>因此，15%的掩码概率在BERT中是一个平衡点，既保证了语义建模的深度，又避免了训练不稳定。</p>
<h2 id="7-5-以情感分类数据集SST-2为例，通过实验论证特征提取和模型精调两种BERT的典-型应用方式对下游任务效果的影响。"><a href="#7-5-以情感分类数据集SST-2为例，通过实验论证特征提取和模型精调两种BERT的典-型应用方式对下游任务效果的影响。" class="headerlink" title="7.5 以情感分类数据集SST-2为例，通过实验论证特征提取和模型精调两种BERT的典 型应用方式对下游任务效果的影响。"></a><del>7.5 以情感分类数据集SST-2为例，通过实验论证特征提取和模型精调两种BERT的典 型应用方式对下游任务效果的影响。</del></h2><h2 id="7-6-在抽取式阅读理解任务中，篇章与问题的拼接顺序会对模型效果产生何种影响？-请以具体的抽取式阅读理解任务CMRC-2018为例进行实验，并给出相应的实验结论。"><a href="#7-6-在抽取式阅读理解任务中，篇章与问题的拼接顺序会对模型效果产生何种影响？-请以具体的抽取式阅读理解任务CMRC-2018为例进行实验，并给出相应的实验结论。" class="headerlink" title="7.6 在抽取式阅读理解任务中，篇章与问题的拼接顺序会对模型效果产生何种影响？ 请以具体的抽取式阅读理解任务CMRC 2018为例进行实验，并给出相应的实验结论。"></a><del>7.6 在抽取式阅读理解任务中，篇章与问题的拼接顺序会对模型效果产生何种影响？ 请以具体的抽取式阅读理解任务CMRC 2018为例进行实验，并给出相应的实验结论。</del></h2><h1 id="第八章"><a href="#第八章" class="headerlink" title="第八章"></a>第八章</h1><h2 id="8-1-阐述自回归语言模型和自编码语言模型的优缺点。"><a href="#8-1-阐述自回归语言模型和自编码语言模型的优缺点。" class="headerlink" title="8.1 阐述自回归语言模型和自编码语言模型的优缺点。"></a>8.1 阐述自回归语言模型和自编码语言模型的优缺点。</h2><p><strong>自回归语言模型</strong></p>
<p><strong>优点</strong>：</p>
<ul>
<li><strong>生成能力突出</strong></li>
<li><strong>结构简单</strong></li>
<li><strong>小样本学习能力强</strong></li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li><strong>缺乏双向上下文</strong></li>
<li><strong>生成质量受限</strong></li>
</ul>
<p><strong>自编码语言模型</strong></p>
<p><strong>优点</strong>：</p>
<ul>
<li><strong>双向上下文建模</strong></li>
<li><strong>任务泛化性</strong></li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li><strong>生成能力较弱</strong></li>
<li><strong>训练复杂度高</strong></li>
</ul>
<h2 id="8-2-阐述词向量因式分解与跨层参数共享对ALBERT模型解码时间的影响。"><a href="#8-2-阐述词向量因式分解与跨层参数共享对ALBERT模型解码时间的影响。" class="headerlink" title="8.2 阐述词向量因式分解与跨层参数共享对ALBERT模型解码时间的影响。"></a>8.2 阐述词向量因式分解与跨层参数共享对ALBERT模型解码时间的影响。</h2><ul>
<li>由于参数量减少，词向量的计算和存储成本降低，从而在解码阶段减少了对词向量的计算负担，间接提升了解码速度。</li>
<li>在解码过程中，共享的参数减少了重复计算，进一步加速了推理过程。</li>
</ul>
<h2 id="8-3-相比传统通过文本切分的方式处理长文本，阐述长文本处理模型处理阅读理解和-命名实体识别任务的优势。"><a href="#8-3-相比传统通过文本切分的方式处理长文本，阐述长文本处理模型处理阅读理解和-命名实体识别任务的优势。" class="headerlink" title="8.3 相比传统通过文本切分的方式处理长文本，阐述长文本处理模型处理阅读理解和 命名实体识别任务的优势。"></a>8.3 相比传统通过文本切分的方式处理长文本，阐述长文本处理模型处理阅读理解和 命名实体识别任务的优势。</h2><ul>
<li><strong>保留上下文信息</strong>：传统方法通过固定长度切分文本，导致块间信息割裂，削弱长距离依赖建模能力（如Transformer-XL通过逐步扩展上下文窗口，使模型在测试阶段能处理更长文本）。</li>
<li><strong>增强全局注意力</strong>：长文本模型引入全局注意力或稀疏注意力机制，使模型能关注更广泛的上下文，提升对长距离依赖的捕捉能力</li>
<li><strong>减少信息丢失</strong>：传统切分方法破坏词与序列标签的一一对应关系，而长文本模型通过优化注意力机制，更准确地建模实体上下文。</li>
</ul>
<h2 id="8-4-仿照8-4-4节中的介绍，尝试构造GPT-3在问答任务上的输入形式。"><a href="#8-4-仿照8-4-4节中的介绍，尝试构造GPT-3在问答任务上的输入形式。" class="headerlink" title="8.4 仿照8.4.4节中的介绍，尝试构造GPT-3在问答任务上的输入形式。"></a><del>8.4 仿照8.4.4节中的介绍，尝试构造GPT-3在问答任务上的输入形式。</del></h2><h2 id="8-5-仿照7-4-2节中的介绍，在SST-2数据集上，使用RoBERTa-base和ELECTRA-base模-型训练单句文本分类模型，并对比两者的实验效果。"><a href="#8-5-仿照7-4-2节中的介绍，在SST-2数据集上，使用RoBERTa-base和ELECTRA-base模-型训练单句文本分类模型，并对比两者的实验效果。" class="headerlink" title="8.5 仿照7.4.2节中的介绍，在SST-2数据集上，使用RoBERTa-base和ELECTRA-base模 型训练单句文本分类模型，并对比两者的实验效果。"></a><del>8.5 仿照7.4.2节中的介绍，在SST-2数据集上，使用RoBERTa-base和ELECTRA-base模 型训练单句文本分类模型，并对比两者的实验效果。</del></h2><h2 id="8-6-在MNLI数据集上，利用TextBrewer工具包实现12层BERT-base-cased模型蒸馏至3-层的BERT模型，要求准确率不低于81"><a href="#8-6-在MNLI数据集上，利用TextBrewer工具包实现12层BERT-base-cased模型蒸馏至3-层的BERT模型，要求准确率不低于81" class="headerlink" title="8.6 在MNLI数据集上，利用TextBrewer工具包实现12层BERT-base-cased模型蒸馏至3 层的BERT模型，要求准确率不低于81%"></a><del>8.6 在MNLI数据集上，利用TextBrewer工具包实现12层BERT-base-cased模型蒸馏至3 层的BERT模型，要求准确率不低于81%</del></h2><h1 id="第九章"><a href="#第九章" class="headerlink" title="第九章"></a>第九章</h1><h2 id="9-1-阐述多语言预训练模型研究的主要意义与应用场景。"><a href="#9-1-阐述多语言预训练模型研究的主要意义与应用场景。" class="headerlink" title="9.1 阐述多语言预训练模型研究的主要意义与应用场景。"></a>9.1 阐述多语言预训练模型研究的主要意义与应用场景。</h2><p><strong>主要意义</strong></p>
<ul>
<li><strong>促进跨语言处理</strong></li>
<li><strong>降低多语言NLP系统的开发成本</strong></li>
<li><strong>提升模型泛化能力</strong></li>
</ul>
<p><strong>主要应用场景</strong></p>
<ul>
<li><strong>零样本迁移</strong></li>
<li><strong>多语言问答与翻译</strong></li>
<li><strong>跨语言信息检索</strong></li>
<li><strong>多语言情感分析与命名实体识别</strong></li>
<li><strong>跨语言文档理解</strong></li>
</ul>
<h2 id="9-2-使用HuggingFace提供的transformers库，分别实现基于mBERT模型与XLM-R模型-的跨语言自然语言推理，并在相应的基准数据集XNLI上进行实验。"><a href="#9-2-使用HuggingFace提供的transformers库，分别实现基于mBERT模型与XLM-R模型-的跨语言自然语言推理，并在相应的基准数据集XNLI上进行实验。" class="headerlink" title="9.2 使用HuggingFace提供的transformers库，分别实现基于mBERT模型与XLM-R模型 的跨语言自然语言推理，并在相应的基准数据集XNLI上进行实验。"></a><del>9.2 使用HuggingFace提供的transformers库，分别实现基于mBERT模型与XLM-R模型 的跨语言自然语言推理，并在相应的基准数据集XNLI上进行实验。</del></h2><h2 id="9-3-试分析多媒体融合的预训练模型目前存在哪些主要的挑战或瓶颈？结合最新的相-关文献说明。"><a href="#9-3-试分析多媒体融合的预训练模型目前存在哪些主要的挑战或瓶颈？结合最新的相-关文献说明。" class="headerlink" title="9.3 试分析多媒体融合的预训练模型目前存在哪些主要的挑战或瓶颈？结合最新的相 关文献说明。"></a>9.3 试分析多媒体融合的预训练模型目前存在哪些主要的挑战或瓶颈？结合最新的相 关文献说明。</h2><ul>
<li><strong>多模态数据对齐与融合的复杂性</strong></li>
<li><strong>数据获取成本高</strong></li>
<li><strong>计算资源需求大</strong></li>
<li><strong>泛化能力与跨任务迁移的局限性</strong></li>
<li><strong>模型可解释性与可控性不足</strong></li>
</ul>
<h2 id="9-4-融合了知识库的预训练模型（如-ERNIE、KnowBERT）存在哪些潜在的缺点？"><a href="#9-4-融合了知识库的预训练模型（如-ERNIE、KnowBERT）存在哪些潜在的缺点？" class="headerlink" title="9.4 融合了知识库的预训练模型（如 ERNIE、KnowBERT）存在哪些潜在的缺点？"></a>9.4 融合了知识库的预训练模型（如 ERNIE、KnowBERT）存在哪些潜在的缺点？</h2><ul>
<li><strong>知识获取与更新成本高</strong></li>
<li><strong>知识融合机制复杂</strong></li>
<li><strong>对知识表示的依赖性强</strong></li>
<li><strong>模型可解释性与可控性受限</strong></li>
<li><strong>数据稀疏性问题</strong></li>
<li><strong>训练效率与资源消耗</strong></li>
</ul>
<h2 id="9-5-除了实体、关系等结构化知识，目前还有哪些知识是预训练模型缺少的或者难以-从文本中直接学习到的？"><a href="#9-5-除了实体、关系等结构化知识，目前还有哪些知识是预训练模型缺少的或者难以-从文本中直接学习到的？" class="headerlink" title="9.5 除了实体、关系等结构化知识，目前还有哪些知识是预训练模型缺少的或者难以 从文本中直接学习到的？"></a>9.5 除了实体、关系等结构化知识，目前还有哪些知识是预训练模型缺少的或者难以 从文本中直接学习到的？</h2><ul>
<li><strong>常识与背景知识</strong></li>
<li><strong>因果关系与逻辑推理</strong></li>
<li><strong>领域特定知识</strong></li>
<li><strong>隐含的语义关系</strong></li>
<li><strong>时间与事件顺序</strong></li>
</ul>
<h2 id="9-6-在多任务或多语言学习的过程中，考虑到不同任务（语言）的数据量、难度往往-不一致，在训练时应当注意哪些问题，以及采用哪些策略？结合-MT-DNN、ERNIE-2-0模-型，以及第8章介绍的T5等模型进行分析。"><a href="#9-6-在多任务或多语言学习的过程中，考虑到不同任务（语言）的数据量、难度往往-不一致，在训练时应当注意哪些问题，以及采用哪些策略？结合-MT-DNN、ERNIE-2-0模-型，以及第8章介绍的T5等模型进行分析。" class="headerlink" title="9.6 在多任务或多语言学习的过程中，考虑到不同任务（语言）的数据量、难度往往 不一致，在训练时应当注意哪些问题，以及采用哪些策略？结合 MT-DNN、ERNIE 2.0模 型，以及第8章介绍的T5等模型进行分析。"></a>9.6 在多任务或多语言学习的过程中，考虑到不同任务（语言）的数据量、难度往往 不一致，在训练时应当注意哪些问题，以及采用哪些策略？结合 MT-DNN、ERNIE 2.0模 型，以及第8章介绍的T5等模型进行分析。</h2><p><strong>1. 分阶段训练与任务优先级调整</strong></p>
<ul>
<li><strong>MT-DNN</strong>通过分阶段训练解决数据量不一致的问题。其先预训练共享编码层，再利用多任务学习阶段结合不同任务的标注数据进行微调。例如，MT-DNN在训练初期仅使用少量任务数据，逐步增加任务数量，避免因数据稀缺导致的模型过拟合。</li>
<li><strong>ERNIE 2.0</strong>采用<strong>连续多任务学习</strong>策略，动态分配任务在各阶段的训练次数。例如，在维持总迭代次数不变的前提下，为任务分配不同的训练权重，使资源更集中在数据量少或难度高的任务上。</li>
</ul>
<p><strong>2. 动态任务权重与正则化</strong></p>
<ul>
<li><strong>T5模型</strong>通过<strong>任务提示（Prompt）技术</strong>统一多任务训练，将不同任务转化为文本生成问题。例如，T5在训练时结合任务描述（如“回答问题”或“翻译句子”）作为输入前缀，使模型无需为每个任务单独设计结构。</li>
<li><strong>ERNIE 2.0</strong>在损失函数中引入任务相关正则化项，防止因任务难度差异导致的灾难性遗忘。例如，在精调阶段同时保留预训练任务的损失，确保模型在下游任务中不丢失通用知识。</li>
</ul>
</article><div class="post-copyright"><div class="copyright-cc-box"><i class="anzhiyufont anzhiyu-icon-copyright"></i></div><div class="post-copyright__author_box"><a class="post-copyright__author_img" href="/" title="头像"><img class="post-copyright__author_img_back" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/personal.jpg" title="头像" alt="头像"><img class="post-copyright__author_img_front" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/personal.jpg" title="头像" alt="头像"></a><div class="post-copyright__author_name">优雅的修勾</div><div class="post-copyright__author_desc">个人博客</div></div><div class="post-copyright__post__info"><a class="post-copyright__original" title="该文章为原创文章，注意版权协议" href="https://love.xiugou.top/posts/nlp_work.html">原创</a><a class="post-copyright-title"><span onclick="rm.copyPageUrl('https://love.xiugou.top/posts/nlp_work.html')">自然语言处理课后题</span></a></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div class="rewardLeftButton"><div class="post-reward" onclick="anzhiyu.addRewardMask()"><div class="reward-button button--animated" title="赞赏作者"><i class="anzhiyufont anzhiyu-icon-hand-heart-fill"></i>打赏作者</div><div class="reward-main"><div class="reward-all"><span class="reward-title">感谢你赐予我前进的力量</span><ul class="reward-group"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul><a class="reward-main-btn" href="/about/#about-reward" target="_blank"><div class="reward-text">赞赏者名单</div><div class="reward-dec">因为你们的支持让我意识到写文章的价值🙏</div></a></div></div></div><div id="quit-box" onclick="anzhiyu.removeRewardMask()" style="display: none"></div></div><div class="shareRight"><div class="share-link mobile"><div class="share-qrcode"><div class="share-button" title="使用手机访问这篇文章"><i class="anzhiyufont anzhiyu-icon-qrcode"></i></div><div class="share-main"><div class="share-main-all"><div id="qrcode" title="https://love.xiugou.top/posts/nlp_work.html"></div><div class="reward-dec">使用手机访问这篇文章</div></div></div></div></div><div class="share-link weibo"><a class="share-button" target="_blank" href="https://service.weibo.com/share/share.php?title=自然语言处理课后题&amp;url=https://love.xiugou.top/posts/nlp_work.html&amp;pic=" rel="external nofollow noreferrer noopener"><i class="anzhiyufont anzhiyu-icon-weibo"></i></a></div><script>function copyCurrentPageUrl() {
  var currentPageUrl = window.location.href;
  var input = document.createElement("input");
  input.setAttribute("value", currentPageUrl);
  document.body.appendChild(input);
  input.select();
  input.setSelectionRange(0, 99999);
  document.execCommand("copy");
  document.body.removeChild(input);
}</script><div class="share-link copyurl"><div class="share-button" id="post-share-url" title="复制链接" onclick="copyCurrentPageUrl()"><i class="anzhiyufont anzhiyu-icon-link"></i></div></div></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://love.xiugou.top" target="_blank">奇幻空间</a>！</span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__box"><div class="post-meta__box__tag-list"></div></div></div><div class="post_share"><div class="social-share" data-image="/personal.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"/><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer="defer"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-full"><a href="/posts/tag-test.html"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">主题标签测试</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="anzhiyufont anzhiyu-icon-comments"></i><span> 评论</span></div><div class="comment-randomInfo"><a onclick="anzhiyu.addRandomCommentInfo()" href="javascript:void(0)">匿名评论</a><a href="/privacy" style="margin-left: 4px">隐私政策</a></div><div class="comment-tips" id="comment-tips"><span>✅ 你无需删除空行，直接评论以获取最佳展示效果</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div><div class="comment-barrage"></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="author-info__sayhi" id="author-info__sayhi" onclick="anzhiyu.changeSayHelloText()"></div><div class="author-info-avatar"><img class="avatar-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/personal.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-status"><img class="g-status" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/08/24/64e6ce9c507bb.png" alt="status"/></div></div><div class="author-info__description">欢迎来到我的个人博客</div><div class="author-info__bottom-group"><a class="author-info__bottom-group-left" href="/"><h1 class="author-info__name">优雅的修勾</h1><div class="author-info__desc">个人博客</div></a><div class="card-info-social-icons is-center"><a class="social-icon faa-parent animated-hover" href="https://github.com/maplesdie" target="_blank" title="Github"></a><a class="social-icon faa-parent animated-hover" href="https://space.bilibili.com/497831655?spm_id_from=333.1007.0.0" target="_blank" title="BiliBili"></a></div></div></div></div><div class="card-widget anzhiyu-right-widget" id="card-wechat" onclick="null"><div id="flip-wrapper"><div id="flip-content"><div class="face" style="background: url(https://bu.dusays.com/2023/01/13/63c02edf44033.png) center center / 100% no-repeat"></div><div class="back face" style="background: url(https://bu.dusays.com/2023/05/13/645fa415e8694.png) center center / 100% no-repeat"></div></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bars"></i><span>文章目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0"><span class="toc-number">1.</span> <span class="toc-text">第二章</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E4%B8%8E%E5%9F%BA%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%E5%88%86%E5%88%AB%E6%9C%89%E5%93%AA%E4%BA%9B%E4%BC%98%E7%BC%BA%E7%82%B9%EF%BC%9F"><span class="toc-number">1.1.</span> <span class="toc-text">2.1 基于规则与基于机器学习的自然语言处理方法分别有哪些优缺点？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E5%A6%82%E4%BD%95%E5%9C%A8%E8%AF%8D%E7%9A%84%E7%8B%AC%E7%83%AD%E8%A1%A8%E7%A4%BA%E4%B8%AD%E5%BC%95%E5%85%A5%E8%AF%8D%E6%80%A7%E3%80%81%E8%AF%8D%E4%B9%89%E7%AD%89%E7%89%B9%E5%BE%81%EF%BC%9F%E8%AF%B7%E4%B8%BE%E4%BE%8B%E8%AF%B4%E6%98%8E%E3%80%82"><span class="toc-number">1.2.</span> <span class="toc-text">2.2 如何在词的独热表示中引入词性、词义等特征？请举例说明。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3%E6%96%B9%E6%B3%95%E6%98%AF%E5%A6%82%E4%BD%95%E5%8F%8D%E6%98%A0%E8%AF%8D%E4%B9%8B%E9%97%B4%E7%9A%84%E9%AB%98%E9%98%B6%E5%85%B3%E7%B3%BB%E7%9A%84%EF%BC%9F"><span class="toc-number">1.3.</span> <span class="toc-text">2.3 奇异值分解方法是如何反映词之间的高阶关系的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-%E5%9C%A8%E4%BD%BF%E7%94%A8%E5%BC%8F%EF%BC%882-18%EF%BC%89%E8%AE%A1%E7%AE%97%E5%9B%B0%E6%83%91%E5%BA%A6%E6%97%B6%EF%BC%8C%E5%A6%82%E6%9E%9C%E5%85%B6%E4%B8%AD%E7%9A%84%E6%9F%90%E4%B8%80%E9%A1%B9%E6%A6%82%E7%8E%87%E4%B8%BA0%EF%BC%8C%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%EF%BC%9F"><span class="toc-number">1.4.</span> <span class="toc-text">2.4 在使用式（2-18）计算困惑度时，如果其中的某一项概率为0，如何处理？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-5-%E8%8B%A5%E4%BD%BF%E7%94%A8%E9%80%86%E5%90%91%E6%9C%80%E5%A4%A7%E5%8C%B9%E9%85%8D%E7%AE%97%E6%B3%95%E5%AF%B9%E5%8F%A5%E5%AD%90%E2%80%9C%E7%A0%94%E7%A9%B6%E7%94%9F%E5%91%BD%E7%9A%84%E8%B5%B7%E6%BA%90%E2%80%9D%E8%BF%9B%E8%A1%8C%E5%88%86%E8%AF%8D%EF%BC%8C%E7%BB%93%E6%9E%9C%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%E6%98%AF"><span class="toc-number">1.5.</span> <span class="toc-text">2.5 若使用逆向最大匹配算法对句子“研究生命的起源”进行分词，结果是什么？是</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-6-2-2-2%E8%8A%82%E4%BB%8B%E7%BB%8D%E7%9A%84%E5%AD%90%E8%AF%8D%E5%88%87%E5%88%86%E7%AE%97%E6%B3%95%E6%98%AF%E5%90%A6%E5%8F%AF%E4%BB%A5%E7%94%A8%E4%BA%8E%E4%B8%AD%E6%96%87%EF%BC%9F%E8%8B%A5%E8%83%BD%E5%BA%94%E7%94%A8%EF%BC%8C%E5%88%99%E4%B8%8E%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E7%9B%B8%E6%AF%94%E6%9C%89%E5%93%AA%E4%BA%9B%E4%BC%98%E7%BC%BA%E7%82%B9%EF%BC%9F"><span class="toc-number">1.6.</span> <span class="toc-text">2.6 2.2.2节介绍的子词切分算法是否可以用于中文？若能应用，则与中文分词相比有哪些优缺点？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-7-%E6%98%AF%E5%90%A6%E5%8F%AF%E4%BB%A5%E4%BD%BF%E7%94%A8%E5%BA%8F%E5%88%97%E6%A0%87%E6%B3%A8%E6%96%B9%E6%B3%95%E8%A7%A3%E5%86%B3%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90%EF%BC%88%E7%9F%AD%E8%AF%AD%E7%BB%93%E6%9E%84%E5%92%8C%E4%BE%9D%E5%AD%98%E4%B8%A4%E7%A7%8D%EF%BC%89%E9%97%AE%E9%A2%98%EF%BC%9F%E8%8B%A5%E8%83%BD%E4%BD%BF%E7%94%A8%EF%BC%8C%E5%88%99%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%EF%BC%9F"><span class="toc-number">1.7.</span> <span class="toc-text">2.7 是否可以使用序列标注方法解决句法分析（短语结构和依存两种）问题？若能使用，则如何进行？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-8-%E4%BD%BF%E7%94%A8%E4%BD%95%E7%A7%8D%E8%AF%84%E4%BB%B7%E6%96%B9%E6%B3%95%E8%AF%84%E4%BB%B7%E4%B8%80%E4%B8%AA%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E7%B3%BB%E7%BB%9F%EF%BC%9F%E5%B9%B6%E8%AF%B7%E7%BC%96%E7%A8%8B%E5%AE%9E%E7%8E%B0%E8%AF%A5%E8%AF%84%E4%BB%B7%E6%96%B9%E6%B3%95%E3%80%82"><span class="toc-number">1.8.</span> <span class="toc-text">2.8 使用何种评价方法评价一个中文分词系统？并请编程实现该评价方法。</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E7%AB%A0"><span class="toc-number">2.</span> <span class="toc-text">第三章</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E4%BD%BF%E7%94%A8NLTK%E5%B7%A5%E5%85%B7%E4%B8%8B%E8%BD%BD%E7%AE%80%C2%B7%E5%A5%A5%E6%96%AF%E6%B1%80%E6%89%80%E8%91%97%E7%9A%84Emma%E5%B0%8F%E8%AF%B4%E5%8E%9F%E6%96%87%EF%BC%8C%E5%B9%B6%E5%8E%BB%E6%8E%89%E5%85%B6%E4%B8%AD%E7%9A%84%E5%81%9C%E7%94%A8%E8%AF%8D%E3%80%82"><span class="toc-number">2.1.</span> <span class="toc-text">3.1 使用NLTK工具下载简·奥斯汀所著的Emma小说原文，并去掉其中的停用词。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E4%BD%BF%E7%94%A8NLTK%E6%8F%90%E4%BE%9B%E7%9A%84WordNet%E8%AE%A1%E7%AE%97%E4%B8%A4%E4%B8%AA%E8%AF%8D%EF%BC%88%E4%B8%8D%E6%98%AF%E8%AF%8D%E4%B9%89%EF%BC%89%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6%EF%BC%8C%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95%E4%B8%BA%E4%B8%A4%E8%AF%8D%E5%90%84%E7%A7%8D%E8%AF%8D%E4%B9%89%E4%B9%8B%E9%97%B4%E7%9A%84%E6%9C%80%E5%A4%A7%E7%9B%B8%E4%BC%BC%E5%BA%A6%E3%80%82"><span class="toc-number">2.2.</span> <span class="toc-text">3.2 使用NLTK提供的WordNet计算两个词（不是词义）的相似度，计算方法为两词各种词义之间的最大相似度。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-%E4%BD%BF%E7%94%A8NLTK%E6%8F%90%E4%BE%9B%E7%9A%84SentiWordNet%E5%B7%A5%E5%85%B7%E8%AE%A1%E7%AE%97%E4%B8%80%E4%B8%AA%E5%8F%A5%E5%AD%90%E7%9A%84%E6%83%85%E6%84%9F%E5%80%BE%E5%90%91%E6%80%A7%EF%BC%8C%E8%AE%A1%E7%AE%97%E6%96%B9%E6%B3%95%E4%B8%BA%E6%AF%8F%E4%B8%AA%E8%AF%8D%E6%89%80%E5%A4%84%E8%AF%8D%E6%80%A7%E4%B8%8B%E7%9A%84%E6%AF%8F%E4%B8%AA%E8%AF%8D%E4%B9%89%E6%83%85%E6%84%9F%E5%80%BE%E5%90%91%E6%80%A7%E4%B9%8B%E5%92%8C%E3%80%82"><span class="toc-number">2.3.</span> <span class="toc-text">3.3 使用NLTK提供的SentiWordNet工具计算一个句子的情感倾向性，计算方法为每个词所处词性下的每个词义情感倾向性之和。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4-%E4%BD%BF%E7%94%A8%E7%9C%9F%E5%AE%9E%E6%96%87%E6%9C%AC%E5%AF%B9%E6%AF%94LTP%E4%B8%8E%E6%AD%A3%E5%90%91%E6%9C%80%E5%A4%A7%E5%8C%B9%E9%85%8D%E5%88%86%E8%AF%8D%E7%9A%84%E7%BB%93%E6%9E%9C%EF%BC%8C%E5%B9%B6%E4%BA%BA%E5%B7%A5%E5%88%86%E6%9E%90%E5%93%AA%E4%BA%9B%E7%BB%93%E6%9E%9CLTP%E6%AD%A3%E7%A1%AE%EF%BC%8C%E6%AD%A3%E5%90%91%E6%9C%80%E5%A4%A7%E5%8C%B9%E9%85%8D%E9%94%99%E8%AF%AF%EF%BC%9B%E5%93%AA%E4%BA%9B%E7%BB%93%E6%9E%9CLTP%E9%94%99%E8%AF%AF%EF%BC%8C%E6%AD%A3%E5%90%91%E6%9C%80%E5%A4%A7%E5%8C%B9%E9%85%8D%E6%AD%A3%E7%A1%AE%EF%BC%9B%E4%BB%A5%E5%8F%8A%E5%93%AA%E4%BA%9B%E7%BB%93%E6%9E%9C%E4%B8%A4%E4%B8%AA%E7%BB%93%E6%9E%9C%E9%83%BD%E9%94%99%E8%AF%AF%E3%80%82"><span class="toc-number">2.4.</span> <span class="toc-text">3.4 使用真实文本对比LTP与正向最大匹配分词的结果，并人工分析哪些结果LTP正确，正向最大匹配错误；哪些结果LTP错误，正向最大匹配正确；以及哪些结果两个结果都错误。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-5-%E5%88%86%E6%9E%90view%E3%80%81reshape%E3%80%81transpose%E5%92%8Cpermute%E5%9B%9B%E7%A7%8D%E8%B0%83%E6%95%B4%E5%BC%A0%E9%87%8F%E5%BD%A2%E7%8A%B6%E6%96%B9%E6%B3%95%E5%90%84%E8%87%AA%E6%93%85%E9%95%BF%E5%A4%84%E7%90%86%E7%9A%84%E9%97%AE%E9%A2%98%E3%80%82"><span class="toc-number">2.5.</span> <span class="toc-text">3.5 分析view、reshape、transpose和permute四种调整张量形状方法各自擅长处理的问题。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-6-%E5%AE%89%E8%A3%85PyTorch%E5%B9%B6%E5%AE%9E%E9%99%85%E5%AF%B9%E6%AF%94%E4%BD%BF%E7%94%A8%E5%92%8C%E4%B8%8D%E4%BD%BF%E7%94%A8GPU%E6%97%B6%EF%BC%8C%E4%B8%89%E4%B8%AA%E5%A4%A7%E5%BC%A0%E9%87%8F%E7%9B%B8%E4%B9%98%E6%97%B6%E7%9A%84%E6%95%88%E7%8E%87%E3%80%82"><span class="toc-number">2.6.</span> <span class="toc-text">3.6 安装PyTorch并实际对比使用和不使用GPU时，三个大张量相乘时的效率。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-7-%E4%B8%8B%E8%BD%BD%E6%9C%80%E6%96%B0%E7%9A%84Common-Crawl%E6%95%B0%E6%8D%AE%EF%BC%8C%E5%B9%B6%E5%AE%9E%E7%8E%B0%E6%8A%BD%E5%8F%96%E4%B8%AD%E6%96%87%E3%80%81%E5%8E%BB%E9%87%8D%E3%80%81%E7%B9%81%E7%AE%80%E8%BD%AC%E6%8D%A2%E3%80%81%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%E7%AD%89%E5%8A%9F%E8%83%BD%E3%80%82"><span class="toc-number">2.7.</span> <span class="toc-text">3.7 下载最新的Common Crawl数据，并实现抽取中文、去重、繁简转换、数据清洗等功能。</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E7%AB%A0"><span class="toc-number">3.</span> <span class="toc-text">第四章</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-%E8%AF%95%E8%AF%81%E6%98%8ESigmoid%E5%87%BD%E6%95%B0%E7%9A%84%E5%AF%BC%E6%95%B0%E4%B8%BA-y%E2%80%B2-y%EF%BC%881%E2%88%92y%EF%BC%89-%E3%80%82"><span class="toc-number">3.1.</span> <span class="toc-text">4.1 试证明Sigmoid函数的导数为$y′&#x3D;y（1−y）$。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E5%BC%8F%EF%BC%884-5%EF%BC%89%E4%B8%AD%EF%BC%8C%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3zi%E8%BF%87%E5%A4%A7%EF%BC%8C%E5%AF%BC%E8%87%B4e%E7%9A%84zi%E6%AC%A1%E6%96%B9%E6%95%B0%E5%80%BC%E6%BA%A2%E5%87%BA%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">3.2.</span> <span class="toc-text">4.2 式（4-5）中，如何解决zi过大，导致e的zi次方数值溢出的问题？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-%E8%8B%A5%E5%8E%BB%E6%8E%89%E5%BC%8F-%EF%BC%884-11%EF%BC%89-%E4%B8%AD%E7%9A%84-ReLU-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%EF%BC%8C%E8%AF%A5%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8%E6%98%AF%E5%90%A6%E8%BF%98%E8%83%BD%E5%A4%84%E7%90%86%E5%BC%82%E6%88%96%E9%97%AE%E9%A2%98%EF%BC%9F%E4%B8%BA%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">3.3.</span> <span class="toc-text">4.3 若去掉式 （4-11） 中的 ReLU 激活函数，该多层感知器是否还能处理异或问题？为什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-4-%E5%9C%A8%E4%BD%BF%E7%94%A8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%97%B6%EF%BC%8C%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E6%9C%89%E7%94%A8%E7%89%B9%E5%BE%81%E9%95%BF%E5%BA%A6%E5%A4%A7%E4%BA%8E%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%AE%BD%E5%BA%A6%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">3.4.</span> <span class="toc-text">4.4 在使用卷积神经网络时，如何解决有用特征长度大于卷积核宽度的问题？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-5-%E5%9C%A8%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%EF%BC%8C%E5%90%84%E6%97%B6%E5%88%BB%E5%85%B1%E4%BA%AB%E6%9D%83%E9%87%8D%E7%9A%84%E6%9C%BA%E5%88%B6%E6%9C%89%E4%BD%95%E4%BC%98%E7%BC%BA%E7%82%B9%EF%BC%9F"><span class="toc-number">3.5.</span> <span class="toc-text">4.5 在循环神经网络中，各时刻共享权重的机制有何优缺点？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-6-%E5%9C%A8%E5%A4%84%E7%90%86%E9%95%BF%E8%B7%9D%E7%A6%BB%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB%E6%97%B6%EF%BC%8C%E5%8E%9F%E5%A7%8B%E7%9A%84%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C%EF%BC%88LSTM%EF%BC%89%E5%9C%A8%E6%9C%BA%E5%88%B6%E4%B8%8A%E6%9C%89%E4%BD%95%E6%9C%AC%E8%B4%A8%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="toc-number">3.6.</span> <span class="toc-text">4.6 在处理长距离依赖关系时，原始的循环神经网络与长短时记忆网络（LSTM）在机制上有何本质的区别？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-7-%E5%9C%A8Transformer%E4%B8%AD%EF%BC%8C%E4%BD%BF%E7%94%A8%E7%BB%9D%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%9A%84%E8%AF%8D%E5%90%91%E9%87%8F%E6%88%96%E7%BC%96%E7%A0%81%E6%9C%89%E4%BD%95%E7%BC%BA%E7%82%B9%EF%BC%9F%E9%92%88%E5%AF%B9%E8%AF%A5%E7%BC%BA%E7%82%B9%E6%9C%89%E4%BD%95%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%EF%BC%9F"><span class="toc-number">3.7.</span> <span class="toc-text">4.7 在Transformer中，使用绝对位置的词向量或编码有何缺点？针对该缺点有何解决方案？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-8-%E5%AE%9E%E9%99%85%E8%BF%90%E8%A1%8C%E6%9C%AC%E7%AB%A0%E5%A4%84%E7%90%86%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E5%92%8C%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8%E9%97%AE%E9%A2%98%E7%9A%84%E4%BB%A3%E7%A0%81%EF%BC%8C%E5%B9%B6%E5%AF%B9%E6%AF%94%E5%90%84%E7%A7%8D%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%87%86%E7%A1%AE%E7%8E%87%EF%BC%8C%E7%84%B6%E5%90%8E%E5%B0%9D%E8%AF%95%E4%BD%BF%E7%94%A8%E5%A4%9A%E7%A7%8D%E6%96%B9%E6%B3%95%E6%8F%90%E9%AB%98%E6%AF%8F%E7%A7%8D%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%87%86%E7%A1%AE%E7%8E%87%E3%80%82"><span class="toc-number">3.8.</span> <span class="toc-text">4.8 实际运行本章处理情感分类和词性标注问题的代码，并对比各种模型的准确率，然后尝试使用多种方法提高每种模型的准确率。</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%BA%94%E7%AB%A0"><span class="toc-number">4.</span> <span class="toc-text">第五章</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-%E5%AE%9E%E9%99%85%E8%BF%90%E8%A1%8C%E6%9C%AC%E7%AB%A0%E6%8F%90%E4%BE%9B%E7%9A%84%E4%B8%8D%E5%90%8C%E8%AF%8D%E5%90%91%E9%87%8F%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E4%BB%A3%E7%A0%81%EF%BC%8C%E8%A7%82%E5%AF%9F%E4%B8%8D%E5%90%8C%E8%B6%85%E5%8F%82%E6%95%B0%E7%9A%84%E8%AE%BE%E7%BD%AE%E5%AF%B9%E4%BA%8E%E8%AF%8D%E5%90%91-%E9%87%8F%E6%80%A7%E8%83%BD%E7%9A%84%E5%BD%B1%E5%93%8D%E3%80%82"><span class="toc-number">4.1.</span> <span class="toc-text">5.1 实际运行本章提供的不同词向量学习模型代码，观察不同超参数的设置对于词向 量性能的影响。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-%E5%9C%A8%E5%9F%BA%E4%BA%8E%E8%B4%9F%E9%87%87%E6%A0%B7%E7%9A%84Skip-gram%E6%A8%A1%E5%9E%8B%E4%B8%AD%EF%BC%8C%E8%AF%95%E5%88%86%E6%9E%90%E4%B8%8D%E5%90%8C%E4%B8%8A%E4%B8%8B%E6%96%87%E7%AA%97%E5%8F%A3%E5%A4%A7%E5%B0%8F%E5%AF%B9%E4%BA%8E%E8%AF%8D%E5%90%91%E9%87%8F%E7%9A%84%E5%BD%B1-%E5%93%8D%EF%BC%8C%E5%88%86%E5%88%AB%E5%9C%A8%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E4%BB%A5%E5%8F%8A%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8%E4%BB%BB%E5%8A%A1%E4%B8%8A%E8%BF%9B%E8%A1%8C%E9%AA%8C%E8%AF%81%E3%80%82"><span class="toc-number">4.2.</span> <span class="toc-text">5.2 在基于负采样的Skip-gram模型中，试分析不同上下文窗口大小对于词向量的影 响，分别在情感分类以及词性标注任务上进行验证。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-%E4%B8%8B%E8%BD%BD%E9%A2%84%E8%AE%AD%E7%BB%83GloVe%E8%AF%8D%E5%90%91%E9%87%8F%EF%BC%8C%E5%88%A9%E7%94%A8t-SNE%E5%AF%B9%E5%85%B6%E8%BF%9B%E8%A1%8C%E5%8F%AF%E8%A7%86%E5%8C%96%E5%88%86%E6%9E%90%E3%80%82"><span class="toc-number">4.3.</span> <span class="toc-text">5.3 下载预训练GloVe词向量，利用t-SNE对其进行可视化分析。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-4-%E5%88%86%E5%88%AB%E4%BB%8E%E8%AF%8D%E8%A1%A8%E7%A4%BA%E4%BB%A5%E5%8F%8A%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E4%B8%A4%E4%B8%AA%E8%A7%92%E5%BA%A6%E5%88%86%E6%9E%90%E9%9D%99%E6%80%81%E8%AF%8D%E5%90%91%E9%87%8F%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9%E3%80%82%E5%B9%B6%E9%92%88%E5%AF%B9%E5%85%B6%E7%BC%BA%E7%82%B9%EF%BC%8C-%E6%80%9D%E8%80%83%E5%B9%B6%E6%8F%90%E5%87%BA%E4%B8%80%E7%A7%8D%E5%90%88%E7%90%86%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E3%80%82"><span class="toc-number">4.4.</span> <span class="toc-text">5.4 分别从词表示以及实际应用两个角度分析静态词向量的优缺点。并针对其缺点， 思考并提出一种合理的解决方案。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-5-%E6%8F%90%E5%87%BA%E4%B8%80%E7%A7%8D%E9%92%88%E5%AF%B9%E4%BD%8E%E9%A2%91%E8%AF%8D%E7%9A%84%E8%AF%8D%E5%90%91%E9%87%8F%E5%AD%A6%E4%B9%A0%E6%94%B9%E8%BF%9B%E6%96%B9%E6%A1%88%E3%80%82"><span class="toc-number">4.5.</span> <span class="toc-text">5.5 提出一种针对低频词的词向量学习改进方案。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-6-%E5%B0%86%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%8D%E5%90%91%E9%87%8F%E7%94%A8%E4%BA%8E%E7%9B%AE%E6%A0%87%E4%BB%BB%E5%8A%A1%E6%97%B6%EF%BC%8C%E5%9C%A8%E4%BB%80%E4%B9%88%E6%83%85%E5%BD%A2%E4%B8%8B%EF%BC%8C%E2%80%9C%E5%86%BB%E7%BB%93%E2%80%9D%E8%AF%8D%E5%90%91%E9%87%8F%E6%AF%94%E7%B2%BE%E8%B0%83%E8%AF%8D%E5%90%91%E9%87%8F-%E6%9B%B4%E5%90%88%E7%90%86%EF%BC%9F%E5%9C%A8%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E4%B8%8A%E8%BF%9B%E8%A1%8C%E9%AA%8C%E8%AF%81%E3%80%82"><span class="toc-number">4.6.</span> <span class="toc-text">5.6 将预训练词向量用于目标任务时，在什么情形下，“冻结”词向量比精调词向量 更合理？在情感分类任务上进行验证。</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%85%AD%E7%AB%A0"><span class="toc-number">5.</span> <span class="toc-text">第六章</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-%E5%88%86%E5%88%AB%E4%BB%8E%E8%AF%8D%E8%A1%A8%E7%A4%BA%E5%92%8C%E8%AF%AD%E4%B9%89%E7%BB%84%E5%90%88%E7%9A%84%E8%A7%92%E5%BA%A6%E9%98%90%E8%BF%B0%E5%8A%A8%E6%80%81%E8%AF%8D%E5%90%91%E9%87%8F%E7%9A%84%E7%89%B9%E7%82%B9%EF%BC%8C%E4%BB%A5%E5%8F%8A%E5%85%B6%E7%9B%B8%E6%AF%94%E4%BA%8E%E9%9D%99%E6%80%81%E8%AF%8D%E5%90%91-%E9%87%8F%E7%9A%84%E4%BC%98%E5%8A%BF%E3%80%82"><span class="toc-number">5.1.</span> <span class="toc-text">6.1 分别从词表示和语义组合的角度阐述动态词向量的特点，以及其相比于静态词向 量的优势。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-%E4%BB%A5%E8%8B%B1%E6%96%87%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E5%A4%9A%E4%B9%89%E8%AF%8D%E2%80%9Cbank%E2%80%9D%E4%B8%BA%E4%BE%8B%EF%BC%8C%E4%BD%BF%E7%94%A8AllenNLP%E6%8F%90%E4%BE%9B%E7%9A%84ELMo%E6%A8%A1%E5%9E%8B%E6%8A%BD%E5%8F%96%E5%85%B6%E5%9C%A8%E4%B8%8D-%E5%90%8C%E5%8F%A5%E5%AD%90%E4%B8%AD%E7%9A%84%E8%AF%8D%E5%90%91%E9%87%8F%EF%BC%8C%E5%B9%B6%E4%BD%BF%E7%94%A8t-SNE%E8%BF%9B%E8%A1%8C%E5%8F%AF%E8%A7%86%E5%8C%96%E5%88%86%E6%9E%90%E3%80%82"><span class="toc-number">5.2.</span> <span class="toc-text">6.2 以英文中常用的多义词“bank”为例，使用AllenNLP提供的ELMo模型抽取其在不 同句子中的词向量，并使用t-SNE进行可视化分析。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-3-%E5%AE%9E%E7%8E%B0%E5%9F%BA%E4%BA%8EELMo%E7%9A%84%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8%EF%BC%8C%E5%B9%B6%E5%AF%B9%E6%AF%94ELMo%E4%B8%8D%E5%90%8C%E5%B1%82%E7%9A%84%E7%89%B9%E5%BE%81%E5%AF%B9%E4%BA%8E%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD%E7%9A%84%E5%BD%B1%E5%93%8D%E3%80%82"><span class="toc-number">5.3.</span> <span class="toc-text">6.3 实现基于ELMo的词性标注，并对比ELMo不同层的特征对于模型性能的影响。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-4-%E4%BD%BF%E7%94%A8Transformer%E7%BB%93%E6%9E%84%E5%AE%9E%E7%8E%B0ELMo%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E5%89%8D%E5%90%91%E3%80%81%E5%90%8E%E5%90%91%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%B9%B6%E5%88%86%E5%88%AB%E4%BB%8E%E8%AF%AD%E8%A8%80%E6%A8%A1-%E5%9E%8B%E5%9B%B0%E6%83%91%E5%BA%A6%E5%92%8C%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E6%80%A7%E8%83%BD%E4%B8%A4%E4%B8%AA%E6%96%B9%E9%9D%A2%E4%B8%8ELSTM%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90%E3%80%82"><span class="toc-number">5.4.</span> <span class="toc-text">6.4 使用Transformer结构实现ELMo模型中的前向、后向语言模型，并分别从语言模 型困惑度和下游任务性能两个方面与LSTM语言模型对比分析。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-5-%E4%B8%BA%E4%BA%86%E8%AE%AD%E7%BB%83%E4%B8%AD%E6%96%87%E7%9A%84ELMo%E6%A8%A1%E5%9E%8B%EF%BC%8C%E9%9C%80%E8%A6%81%E5%AF%B9%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E5%81%9A%E5%93%AA%E4%BA%9B%E8%B0%83%E6%95%B4%EF%BC%9F"><span class="toc-number">5.5.</span> <span class="toc-text">6.5 为了训练中文的ELMo模型，需要对模型结构做哪些调整？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-6-%E9%99%A4%E4%BA%86%E4%BB%A5%E7%89%B9%E5%BE%81%E5%BD%A2%E5%BC%8F%E5%BA%94%E7%94%A8%E4%BA%8E%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%EF%BC%8C%E5%8A%A8%E6%80%81%E8%AF%8D%E5%90%91%E9%87%8F%E8%BF%98%E6%9C%89%E5%93%AA%E4%BA%9B%E6%BD%9C%E5%9C%A8%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%EF%BC%9F"><span class="toc-number">5.6.</span> <span class="toc-text">6.6 除了以特征形式应用于下游任务，动态词向量还有哪些潜在的应用场景？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%83%E7%AB%A0"><span class="toc-number">6.</span> <span class="toc-text">第七章</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#7-1-%E4%BB%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%A7%92%E5%BA%A6%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90GPT%E5%92%8CBERT%E5%90%84%E8%87%AA%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-number">6.1.</span> <span class="toc-text">7.1 从模型的角度对比分析GPT和BERT各自的优缺点是什么？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-2-%E9%98%90%E8%BF%B0BERT%E7%9A%84%E8%BE%93%E5%85%A5%E8%A1%A8%E7%A4%BA%E4%B8%AD%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%8C%85%E5%90%AB%E4%BD%8D%E7%BD%AE%E5%90%91%E9%87%8F%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%B2%A1%E6%9C%89%E4%BD%8D%E7%BD%AE%E5%90%91%E9%87%8F%E5%B0%86%E6%9C%89%E4%BD%95%E5%BD%B1%E5%93%8D%EF%BC%9F"><span class="toc-number">6.2.</span> <span class="toc-text">7.2 阐述BERT的输入表示中为什么要包含位置向量？如果没有位置向量将有何影响？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-3-%E9%98%90%E8%BF%B0%E5%BA%94%E7%94%A8%E4%B8%89%E7%A7%8D%E4%B8%8D%E5%90%8C%E6%8E%A9%E7%A0%81%E7%AD%96%E7%95%A5%EF%BC%88MLM%E3%80%81WWM%E5%92%8CNM%EF%BC%89%E7%9A%84BERT%EF%BC%8C%E5%9C%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E9%98%B6%E6%AE%B5%E5%92%8C%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1-%E7%B2%BE%E8%B0%83%E4%B8%AD%E7%9A%84%E5%BC%82%E5%90%8C%E7%82%B9%E3%80%82"><span class="toc-number">6.3.</span> <span class="toc-text">7.3 阐述应用三种不同掩码策略（MLM、WWM和NM）的BERT，在预训练阶段和下游任务 精调中的异同点。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-4-BERT%E4%B8%AD%E7%9A%84MLM%E9%A2%84%E8%AE%AD%E7%BB%83%E4%BB%BB%E5%8A%A1%E9%87%87%E7%94%A8%E4%BA%8615-%E7%9A%84%E6%8E%A9%E7%A0%81%E6%A6%82%E7%8E%87%EF%BC%8C%E8%AF%B7%E9%98%90%E8%BF%B0%E5%A2%9E%E5%A4%A7%E6%88%96%E5%87%8F%E5%B0%8F%E6%8E%A9%E7%A0%81%E6%A6%82%E7%8E%87%E5%AF%B9%E9%A2%84-%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%88%E6%9E%9C%E5%8F%AF%E8%83%BD%E4%BA%A7%E7%94%9F%E7%9A%84%E5%BD%B1%E5%93%8D%E3%80%82"><span class="toc-number">6.4.</span> <span class="toc-text">7.4 BERT中的MLM预训练任务采用了15%的掩码概率，请阐述增大或减小掩码概率对预 训练语言模型效果可能产生的影响。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-5-%E4%BB%A5%E6%83%85%E6%84%9F%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86SST-2%E4%B8%BA%E4%BE%8B%EF%BC%8C%E9%80%9A%E8%BF%87%E5%AE%9E%E9%AA%8C%E8%AE%BA%E8%AF%81%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%92%8C%E6%A8%A1%E5%9E%8B%E7%B2%BE%E8%B0%83%E4%B8%A4%E7%A7%8DBERT%E7%9A%84%E5%85%B8-%E5%9E%8B%E5%BA%94%E7%94%A8%E6%96%B9%E5%BC%8F%E5%AF%B9%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E6%95%88%E6%9E%9C%E7%9A%84%E5%BD%B1%E5%93%8D%E3%80%82"><span class="toc-number">6.5.</span> <span class="toc-text">7.5 以情感分类数据集SST-2为例，通过实验论证特征提取和模型精调两种BERT的典 型应用方式对下游任务效果的影响。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-6-%E5%9C%A8%E6%8A%BD%E5%8F%96%E5%BC%8F%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E4%BB%BB%E5%8A%A1%E4%B8%AD%EF%BC%8C%E7%AF%87%E7%AB%A0%E4%B8%8E%E9%97%AE%E9%A2%98%E7%9A%84%E6%8B%BC%E6%8E%A5%E9%A1%BA%E5%BA%8F%E4%BC%9A%E5%AF%B9%E6%A8%A1%E5%9E%8B%E6%95%88%E6%9E%9C%E4%BA%A7%E7%94%9F%E4%BD%95%E7%A7%8D%E5%BD%B1%E5%93%8D%EF%BC%9F-%E8%AF%B7%E4%BB%A5%E5%85%B7%E4%BD%93%E7%9A%84%E6%8A%BD%E5%8F%96%E5%BC%8F%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E4%BB%BB%E5%8A%A1CMRC-2018%E4%B8%BA%E4%BE%8B%E8%BF%9B%E8%A1%8C%E5%AE%9E%E9%AA%8C%EF%BC%8C%E5%B9%B6%E7%BB%99%E5%87%BA%E7%9B%B8%E5%BA%94%E7%9A%84%E5%AE%9E%E9%AA%8C%E7%BB%93%E8%AE%BA%E3%80%82"><span class="toc-number">6.6.</span> <span class="toc-text">7.6 在抽取式阅读理解任务中，篇章与问题的拼接顺序会对模型效果产生何种影响？ 请以具体的抽取式阅读理解任务CMRC 2018为例进行实验，并给出相应的实验结论。</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%85%AB%E7%AB%A0"><span class="toc-number">7.</span> <span class="toc-text">第八章</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#8-1-%E9%98%90%E8%BF%B0%E8%87%AA%E5%9B%9E%E5%BD%92%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%92%8C%E8%87%AA%E7%BC%96%E7%A0%81%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9%E3%80%82"><span class="toc-number">7.1.</span> <span class="toc-text">8.1 阐述自回归语言模型和自编码语言模型的优缺点。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-2-%E9%98%90%E8%BF%B0%E8%AF%8D%E5%90%91%E9%87%8F%E5%9B%A0%E5%BC%8F%E5%88%86%E8%A7%A3%E4%B8%8E%E8%B7%A8%E5%B1%82%E5%8F%82%E6%95%B0%E5%85%B1%E4%BA%AB%E5%AF%B9ALBERT%E6%A8%A1%E5%9E%8B%E8%A7%A3%E7%A0%81%E6%97%B6%E9%97%B4%E7%9A%84%E5%BD%B1%E5%93%8D%E3%80%82"><span class="toc-number">7.2.</span> <span class="toc-text">8.2 阐述词向量因式分解与跨层参数共享对ALBERT模型解码时间的影响。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-3-%E7%9B%B8%E6%AF%94%E4%BC%A0%E7%BB%9F%E9%80%9A%E8%BF%87%E6%96%87%E6%9C%AC%E5%88%87%E5%88%86%E7%9A%84%E6%96%B9%E5%BC%8F%E5%A4%84%E7%90%86%E9%95%BF%E6%96%87%E6%9C%AC%EF%BC%8C%E9%98%90%E8%BF%B0%E9%95%BF%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B%E5%A4%84%E7%90%86%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E5%92%8C-%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%E4%BB%BB%E5%8A%A1%E7%9A%84%E4%BC%98%E5%8A%BF%E3%80%82"><span class="toc-number">7.3.</span> <span class="toc-text">8.3 相比传统通过文本切分的方式处理长文本，阐述长文本处理模型处理阅读理解和 命名实体识别任务的优势。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-4-%E4%BB%BF%E7%85%A78-4-4%E8%8A%82%E4%B8%AD%E7%9A%84%E4%BB%8B%E7%BB%8D%EF%BC%8C%E5%B0%9D%E8%AF%95%E6%9E%84%E9%80%A0GPT-3%E5%9C%A8%E9%97%AE%E7%AD%94%E4%BB%BB%E5%8A%A1%E4%B8%8A%E7%9A%84%E8%BE%93%E5%85%A5%E5%BD%A2%E5%BC%8F%E3%80%82"><span class="toc-number">7.4.</span> <span class="toc-text">8.4 仿照8.4.4节中的介绍，尝试构造GPT-3在问答任务上的输入形式。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-5-%E4%BB%BF%E7%85%A77-4-2%E8%8A%82%E4%B8%AD%E7%9A%84%E4%BB%8B%E7%BB%8D%EF%BC%8C%E5%9C%A8SST-2%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%EF%BC%8C%E4%BD%BF%E7%94%A8RoBERTa-base%E5%92%8CELECTRA-base%E6%A8%A1-%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%8D%95%E5%8F%A5%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%B9%B6%E5%AF%B9%E6%AF%94%E4%B8%A4%E8%80%85%E7%9A%84%E5%AE%9E%E9%AA%8C%E6%95%88%E6%9E%9C%E3%80%82"><span class="toc-number">7.5.</span> <span class="toc-text">8.5 仿照7.4.2节中的介绍，在SST-2数据集上，使用RoBERTa-base和ELECTRA-base模 型训练单句文本分类模型，并对比两者的实验效果。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-6-%E5%9C%A8MNLI%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%EF%BC%8C%E5%88%A9%E7%94%A8TextBrewer%E5%B7%A5%E5%85%B7%E5%8C%85%E5%AE%9E%E7%8E%B012%E5%B1%82BERT-base-cased%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F%E8%87%B33-%E5%B1%82%E7%9A%84BERT%E6%A8%A1%E5%9E%8B%EF%BC%8C%E8%A6%81%E6%B1%82%E5%87%86%E7%A1%AE%E7%8E%87%E4%B8%8D%E4%BD%8E%E4%BA%8E81"><span class="toc-number">7.6.</span> <span class="toc-text">8.6 在MNLI数据集上，利用TextBrewer工具包实现12层BERT-base-cased模型蒸馏至3 层的BERT模型，要求准确率不低于81%</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B9%9D%E7%AB%A0"><span class="toc-number">8.</span> <span class="toc-text">第九章</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#9-1-%E9%98%90%E8%BF%B0%E5%A4%9A%E8%AF%AD%E8%A8%80%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%A0%94%E7%A9%B6%E7%9A%84%E4%B8%BB%E8%A6%81%E6%84%8F%E4%B9%89%E4%B8%8E%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%E3%80%82"><span class="toc-number">8.1.</span> <span class="toc-text">9.1 阐述多语言预训练模型研究的主要意义与应用场景。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-2-%E4%BD%BF%E7%94%A8HuggingFace%E6%8F%90%E4%BE%9B%E7%9A%84transformers%E5%BA%93%EF%BC%8C%E5%88%86%E5%88%AB%E5%AE%9E%E7%8E%B0%E5%9F%BA%E4%BA%8EmBERT%E6%A8%A1%E5%9E%8B%E4%B8%8EXLM-R%E6%A8%A1%E5%9E%8B-%E7%9A%84%E8%B7%A8%E8%AF%AD%E8%A8%80%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E6%8E%A8%E7%90%86%EF%BC%8C%E5%B9%B6%E5%9C%A8%E7%9B%B8%E5%BA%94%E7%9A%84%E5%9F%BA%E5%87%86%E6%95%B0%E6%8D%AE%E9%9B%86XNLI%E4%B8%8A%E8%BF%9B%E8%A1%8C%E5%AE%9E%E9%AA%8C%E3%80%82"><span class="toc-number">8.2.</span> <span class="toc-text">9.2 使用HuggingFace提供的transformers库，分别实现基于mBERT模型与XLM-R模型 的跨语言自然语言推理，并在相应的基准数据集XNLI上进行实验。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-3-%E8%AF%95%E5%88%86%E6%9E%90%E5%A4%9A%E5%AA%92%E4%BD%93%E8%9E%8D%E5%90%88%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9B%AE%E5%89%8D%E5%AD%98%E5%9C%A8%E5%93%AA%E4%BA%9B%E4%B8%BB%E8%A6%81%E7%9A%84%E6%8C%91%E6%88%98%E6%88%96%E7%93%B6%E9%A2%88%EF%BC%9F%E7%BB%93%E5%90%88%E6%9C%80%E6%96%B0%E7%9A%84%E7%9B%B8-%E5%85%B3%E6%96%87%E7%8C%AE%E8%AF%B4%E6%98%8E%E3%80%82"><span class="toc-number">8.3.</span> <span class="toc-text">9.3 试分析多媒体融合的预训练模型目前存在哪些主要的挑战或瓶颈？结合最新的相 关文献说明。</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-4-%E8%9E%8D%E5%90%88%E4%BA%86%E7%9F%A5%E8%AF%86%E5%BA%93%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%A6%82-ERNIE%E3%80%81KnowBERT%EF%BC%89%E5%AD%98%E5%9C%A8%E5%93%AA%E4%BA%9B%E6%BD%9C%E5%9C%A8%E7%9A%84%E7%BC%BA%E7%82%B9%EF%BC%9F"><span class="toc-number">8.4.</span> <span class="toc-text">9.4 融合了知识库的预训练模型（如 ERNIE、KnowBERT）存在哪些潜在的缺点？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-5-%E9%99%A4%E4%BA%86%E5%AE%9E%E4%BD%93%E3%80%81%E5%85%B3%E7%B3%BB%E7%AD%89%E7%BB%93%E6%9E%84%E5%8C%96%E7%9F%A5%E8%AF%86%EF%BC%8C%E7%9B%AE%E5%89%8D%E8%BF%98%E6%9C%89%E5%93%AA%E4%BA%9B%E7%9F%A5%E8%AF%86%E6%98%AF%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%BC%BA%E5%B0%91%E7%9A%84%E6%88%96%E8%80%85%E9%9A%BE%E4%BB%A5-%E4%BB%8E%E6%96%87%E6%9C%AC%E4%B8%AD%E7%9B%B4%E6%8E%A5%E5%AD%A6%E4%B9%A0%E5%88%B0%E7%9A%84%EF%BC%9F"><span class="toc-number">8.5.</span> <span class="toc-text">9.5 除了实体、关系等结构化知识，目前还有哪些知识是预训练模型缺少的或者难以 从文本中直接学习到的？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-6-%E5%9C%A8%E5%A4%9A%E4%BB%BB%E5%8A%A1%E6%88%96%E5%A4%9A%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0%E7%9A%84%E8%BF%87%E7%A8%8B%E4%B8%AD%EF%BC%8C%E8%80%83%E8%99%91%E5%88%B0%E4%B8%8D%E5%90%8C%E4%BB%BB%E5%8A%A1%EF%BC%88%E8%AF%AD%E8%A8%80%EF%BC%89%E7%9A%84%E6%95%B0%E6%8D%AE%E9%87%8F%E3%80%81%E9%9A%BE%E5%BA%A6%E5%BE%80%E5%BE%80-%E4%B8%8D%E4%B8%80%E8%87%B4%EF%BC%8C%E5%9C%A8%E8%AE%AD%E7%BB%83%E6%97%B6%E5%BA%94%E5%BD%93%E6%B3%A8%E6%84%8F%E5%93%AA%E4%BA%9B%E9%97%AE%E9%A2%98%EF%BC%8C%E4%BB%A5%E5%8F%8A%E9%87%87%E7%94%A8%E5%93%AA%E4%BA%9B%E7%AD%96%E7%95%A5%EF%BC%9F%E7%BB%93%E5%90%88-MT-DNN%E3%80%81ERNIE-2-0%E6%A8%A1-%E5%9E%8B%EF%BC%8C%E4%BB%A5%E5%8F%8A%E7%AC%AC8%E7%AB%A0%E4%BB%8B%E7%BB%8D%E7%9A%84T5%E7%AD%89%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E5%88%86%E6%9E%90%E3%80%82"><span class="toc-number">8.6.</span> <span class="toc-text">9.6 在多任务或多语言学习的过程中，考虑到不同任务（语言）的数据量、难度往往 不一致，在训练时应当注意哪些问题，以及采用哪些策略？结合 MT-DNN、ERNIE 2.0模 型，以及第8章介绍的T5等模型进行分析。</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-history"></i><span>最近发布</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="https://love.xiugou.top/posts/nlp_work.html" title="自然语言处理课后题">自然语言处理课后题</a><time datetime="2025-06-03T14:04:23.200Z" title="发表于 2025-06-03 22:04:23">2025-06-03</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="https://love.xiugou.top/posts/tag-test.html" title="主题标签测试">主题标签测试</a><time datetime="2025-05-30T16:00:08.640Z" title="发表于 2025-05-31 00:00:08">2025-05-31</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="https://love.xiugou.top/posts/bigdata.html" title="大数据复习">大数据复习</a><time datetime="2025-05-30T11:21:36.170Z" title="发表于 2025-05-30 19:21:36">2025-05-30</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="https://love.xiugou.top/posts/HEXO%E5%90%8E%E5%8F%B0%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F.html" title="HEXO后台管理系统">HEXO后台管理系统</a><time datetime="2025-05-29T07:11:00.000Z" title="发表于 2025-05-29 15:11:00">2025-05-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="https://love.xiugou.top/posts/2025-2-15.html" title="记录第一篇自己写的博客">记录第一篇自己写的博客</a><time datetime="2025-02-14T16:23:10.000Z" title="发表于 2025-02-15 00:23:10">2025-02-15</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div id="footer_deal"><a class="deal_link" href="mailto:2823633196@qq.com" title="email"><i class="anzhiyufont anzhiyu-icon-envelope"></i></a><a class="deal_link" target="_blank" rel="noopener" href="https://weibo.com/" title="微博"><i class="anzhiyufont anzhiyu-icon-weibo"></i></a><a class="deal_link" target="_blank" rel="noopener" href="https://www.facebook.com/" title="facebook"><i class="anzhiyufont anzhiyu-icon-facebook1"></i></a><a class="deal_link" href="/atom.xml" title="RSS"><i class="anzhiyufont anzhiyu-icon-rss"></i></a><img class="footer_mini_logo" title="返回顶部" alt="返回顶部" onclick="anzhiyu.scrollToDest(0, 500)" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/personal.jpg" size="50px"/><a class="deal_link" target="_blank" rel="noopener" href="https://github.com/maplesdie" title="Github"><i class="anzhiyufont anzhiyu-icon-github"></i></a><a class="deal_link" target="_blank" rel="noopener" href="https://space.bilibili.com/497831655?spm_id_from=333.1007.0.0" title="Bilibili"><i class="anzhiyufont anzhiyu-icon-bilibili"></i></a><a class="deal_link" target="_blank" rel="noopener" href="https://www.douyin.com/user/MS4wLjABAAAAE4OfzEvRUA8BF7aIsBpOFJtW4ux6ZdM7DQtF4fRTifs?from_tab_name=main" title="抖音"><i class="anzhiyufont anzhiyu-icon-tiktok"></i></a><a class="deal_link" href="/copyright" title="CC"><i class="anzhiyufont anzhiyu-icon-copyright-line"></i></a></div><div id="workboard"><img class="workSituationImg boardsign" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@2.0.4/img/badge/安知鱼-上班摸鱼中.svg" alt="距离月入25k也就还差一个大佬带我~" title="距离月入25k也就还差一个大佬带我~"/><div id="runtimeTextTip"></div></div><div id="anzhiyu-footer"><div class="footer-group"><div class="footer-title">服务</div><div class="footer-links"><a class="footer-item" title="51la统计" target="_blank" rel="noopener" href="https://v6.51.la/">51la统计</a><a class="footer-item" title="十年之约" target="_blank" rel="noopener" href="https://www.foreverblog.cn/">十年之约</a><a class="footer-item" title="开往" target="_blank" rel="noopener" href="https://github.com/travellings-link/travellings">开往</a></div></div><div class="footer-group"><div class="footer-title">主题</div><div class="footer-links"><a class="footer-item" title="文档" href="/docs/">文档</a><a class="footer-item" title="源码" target="_blank" rel="noopener" href="https://github.com/anzhiyu-c/hexo-theme-anzhiyu">源码</a><a class="footer-item" title="更新日志" href="/update/">更新日志</a></div></div><div class="footer-group"><div class="footer-title">导航</div><div class="footer-links"><a class="footer-item" title="即刻短文" href="/essay/">即刻短文</a><a class="footer-item" title="友链文章" href="/fcircle/">友链文章</a><a class="footer-item" title="留言板" href="/comments/">留言板</a></div></div><div class="footer-group"><div class="footer-title">协议</div><div class="footer-links"><a class="footer-item" title="隐私协议" href="/privacy/">隐私协议</a><a class="footer-item" title="Cookies" href="/cookies/">Cookies</a><a class="footer-item" title="版权协议" href="/copyright/">版权协议</a></div></div><div class="footer-group"><div class="footer-title-group"><div class="footer-title">友链</div><a class="random-friends-btn" id="footer-random-friends-btn" href="javascript:addFriendLinksInFooter();" title="换一批友情链接"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right"></i></a></div><div class="footer-links" id="friend-links-in-footer"></div></div></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo_v5.4.0" title="博客框架为Hexo_v5.4.0"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@2.1.5/img/badge/Frame-Hexo.svg" alt="博客框架为Hexo_v5.4.0"/></a><a class="github-badge" target="_blank" href="https://blog.anheyu.com/" style="margin-inline:5px" data-title="本站使用AnZhiYu主题" title="本站使用AnZhiYu主题"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.9/img/Theme-AnZhiYu-2E67D3.svg" alt="本站使用AnZhiYu主题"/></a><a class="github-badge" target="_blank" href="https://www.dogecloud.com/" style="margin-inline:5px" data-title="本站使用多吉云为静态资源提供CDN加速" title="本站使用多吉云为静态资源提供CDN加速"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@2.2.0/img/badge/CDN-多吉云-3693F3.svg" alt="本站使用多吉云为静态资源提供CDN加速"/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title="本站项目由Github托管"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@2.1.5/img/badge/Source-Github.svg" alt="本站项目由Github托管"/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@2.2.0/img/badge/Copyright-BY-NC-SA.svg" alt="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"/></a></p></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div id="footer-bar-tips"><div class="copyright">&copy;2025 By <a class="footer-bar-link" href="/" title="优雅的修勾" target="_blank">优雅的修勾</a></div></div><div id="footer-type-tips"></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://github.com/anzhiyu-c/hexo-theme-anzhiyu" title="主题">主题</a></div></div></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/archives/" title="archive"><div class="headline">文章</div><div class="length-num">9</div></a><a href="/tags/" title="tag"><div class="headline">标签</div><div class="length-num">6</div></a><a href="/categories/" title="category"><div class="headline">分类</div><div class="length-num">1</div></a></div><span class="sidebar-menu-item-title">功能</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" title="显示模式"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>显示模式</span></a></div><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://blog.anheyu.com/" title="博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/favicon.ico" alt="博客"/><span class="back-menu-item-text">博客</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://image.anheyu.com/" title="安知鱼图床"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="安知鱼图床"/><span class="back-menu-item-text">安知鱼图床</span></a></div></div></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 隧道</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 友链</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/link/"><i class="anzhiyufont anzhiyu-icon-link faa-tada" style="font-size: 0.9em;"></i><span> 友人帐</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/fcircle/"><i class="anzhiyufont anzhiyu-icon-artstation faa-tada" style="font-size: 0.9em;"></i><span> 朋友圈</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><i class="anzhiyufont anzhiyu-icon-envelope faa-tada" style="font-size: 0.9em;"></i><span> 留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/music/"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> 音乐馆</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/bangumis/"><i class="anzhiyufont anzhiyu-icon-bilibili faa-tada" style="font-size: 0.9em;"></i><span> 追番页</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/album/"><i class="anzhiyufont anzhiyu-icon-images faa-tada" style="font-size: 0.9em;"></i><span> 相册集</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/air-conditioner/"><i class="anzhiyufont anzhiyu-icon-fan faa-tada" style="font-size: 0.9em;"></i><span> 小空调</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 关于</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/about/"><i class="anzhiyufont anzhiyu-icon-paper-plane faa-tada" style="font-size: 0.9em;"></i><span> 关于本人</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/essay/"><i class="anzhiyufont anzhiyu-icon-lightbulb faa-tada" style="font-size: 0.9em;"></i><span> 闲言碎语</span></a></li><li><a class="site-page child faa-parent animated-hover" href="javascript:toRandomPost()"><i class="anzhiyufont anzhiyu-icon-shoe-prints1 faa-tada" style="font-size: 0.9em;"></i><span> 随便逛逛</span></a></li></ul></div></div><span class="sidebar-menu-item-title">标签</span><div class="card-tags"><div class="item-headline"></div><div class="card-tag-cloud"><a href="/tags/%E4%BA%BA%E7%94%9F%E8%A7%84%E5%88%92/" style="font-size: 0.88rem;">人生规划<sup>1</sup></a><a href="/tags/%E5%86%B3%E5%AE%9A/" style="font-size: 0.88rem;">决定<sup>1</sup></a><a href="/tags/%E6%95%B0%E5%AD%A6/" style="font-size: 0.88rem;">数学<sup>1</sup></a><a href="/tags/%E8%80%83%E7%A0%94/" style="font-size: 0.88rem;">考研<sup>3</sup></a><a href="/tags/%E8%A7%84%E5%88%92/" style="font-size: 0.88rem;">规划<sup>1</sup></a><a href="/tags/%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/" style="font-size: 0.88rem;">高等数学<sup>1</sup></a></div></div><hr/></div></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="anzhiyufont anzhiyu-icon-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="anzhiyufont anzhiyu-icon-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="anzhiyufont anzhiyu-icon-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div id="nav-music"><a id="nav-music-hoverTips" onclick="anzhiyu.musicToggle()" accesskey="m">播放音乐</a><div id="console-music-bg"></div><meting-js id="13304026559" server="netease" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random" volume="0.7"></meting-js></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="anzhiyufont anzhiyu-icon-xmark"></i></button></nav><div class="is-center" id="loading-database"><i class="anzhiyufont anzhiyu-icon-spinner anzhiyu-pulse-icon"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size: 1rem;"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>粘贴文本</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>引用到评论</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开图片</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>百度搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://music.163.com/#/my/m/music/playlist?id=13304026559&quot;, &quot;_blank&quot;);" style="display: none;"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>查看所有歌曲</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>博客分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制地址</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">关闭热评</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">深色模式</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-language"></i><span>轉為繁體</span></a></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.umd.js"></script><script src="https://cdn.cbd.int/instant.page@5.2.0/instantpage.js" type="module"></script><script src="https://cdn.cbd.int/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script src="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.js"></script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script>// 消除控制台打印
var HoldLog = console.log;
console.log = function () {};
let now1 = new Date();
queueMicrotask(() => {
  const Log = function () {
    HoldLog.apply(console, arguments);
  }; //在恢复前输出日志
  const grt = new Date("2/10/2025 00:00:00"); //此处修改你的建站时间或者网站上线时间
  now1.setTime(now1.getTime() + 250);
  const days = (now1 - grt) / 1000 / 60 / 60 / 24;
  const dnum = Math.floor(days);
  const ascll = [
    `欢迎使用安知鱼!`,
    `生活明朗, 万物可爱`,
    `
        
       █████╗ ███╗   ██╗███████╗██╗  ██╗██╗██╗   ██╗██╗   ██╗
      ██╔══██╗████╗  ██║╚══███╔╝██║  ██║██║╚██╗ ██╔╝██║   ██║
      ███████║██╔██╗ ██║  ███╔╝ ███████║██║ ╚████╔╝ ██║   ██║
      ██╔══██║██║╚██╗██║ ███╔╝  ██╔══██║██║  ╚██╔╝  ██║   ██║
      ██║  ██║██║ ╚████║███████╗██║  ██║██║   ██║   ╚██████╔╝
      ╚═╝  ╚═╝╚═╝  ╚═══╝╚══════╝╚═╝  ╚═╝╚═╝   ╚═╝    ╚═════╝
        
        `,
    "已上线",
    dnum,
    "天",
    "©2025 By 安知鱼 V1.6.14",
  ];
  const ascll2 = [`NCC2-036`, `调用前置摄像头拍照成功，识别为【小笨蛋】.`, `Photo captured: `, `🤪`];

  setTimeout(
    Log.bind(
      console,
      `\n%c${ascll[0]} %c ${ascll[1]} %c ${ascll[2]} %c${ascll[3]}%c ${ascll[4]}%c ${ascll[5]}\n\n%c ${ascll[6]}\n`,
      "color:#425AEF",
      "",
      "color:#425AEF",
      "color:#425AEF",
      "",
      "color:#425AEF",
      ""
    )
  );
  setTimeout(
    Log.bind(
      console,
      `%c ${ascll2[0]} %c ${ascll2[1]} %c \n${ascll2[2]} %c\n${ascll2[3]}\n`,
      "color:white; background-color:#4fd953",
      "",
      "",
      'background:url("https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/tinggge.gif") no-repeat;font-size:450%'
    )
  );

  setTimeout(Log.bind(console, "%c WELCOME %c 你好，小笨蛋.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(
      console,
      "%c ⚡ Powered by 安知鱼 %c 你正在访问 优雅的修勾 的博客.",
      "color:white; background-color:#f0ad4e",
      ""
    )
  );

  setTimeout(Log.bind(console, "%c W23-12 %c 你已打开控制台.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(console, "%c S013-782 %c 你现在正处于监控中.", "color:white; background-color:#d9534f", "")
  );
});</script><script async src="/anzhiyu/random.js"></script><script async="async">(function () {
  var grt = new Date("2/10/2025 00:00:00"); //设置网站上线时间
  var now = new Date();
  var dnum;
  var hnum;
  var mnum;
  var snum;
  var nowHour;

  // 计算并更新天数、小时数、分钟数和秒数
  function updateTime() {
    now = new Date(); // 更新 now 的值
    nowHour = now.getHours(); // 更新 nowHour 的值
    var days = (now - grt) / 1000 / 60 / 60 / 24;
    dnum = Math.floor(days);
    var hours = (now - grt) / 1000 / 60 / 60 - 24 * dnum;
    hnum = Math.floor(hours);
    if (String(hnum).length == 1) {
      hnum = "0" + hnum;
    }
    var minutes = (now - grt) / 1000 / 60 - 24 * 60 * dnum - 60 * hnum;
    mnum = Math.floor(minutes);
    if (String(mnum).length == 1) {
      mnum = "0" + mnum;
    }
    var seconds = (now - grt) / 1000 - 24 * 60 * 60 * dnum - 60 * 60 * hnum - 60 * mnum;
    snum = Math.round(seconds);
    if (String(snum).length == 1) {
      snum = "0" + snum;
    }
  }

  // 更新网页中显示的网站运行时间
  function updateHtml() {
    const footer = document.getElementById("footer");
    if (!footer) return
    let currentTimeHtml = "";
    if (nowHour < 18 && nowHour >= 9) {
      // 如果是上班时间，默认就是"安知鱼-上班摸鱼中.svg"图片，不需要更改
      currentTimeHtml = `本站居然运行了 ${dnum} 天<span id='runtime'> ${hnum} 小时 ${mnum} 分 ${snum} 秒 </span><i class='anzhiyufont anzhiyu-icon-heartbeat' style='color:red'></i>`;
    } else {
      // 如果是下班时间，插入"安知鱼-下班啦.svg"图片
      let img = document.querySelector("#workboard .workSituationImg");
      if (img != null) {
        img.src = "https://npm.elemecdn.com/anzhiyu-blog@2.0.4/img/badge/安知鱼-下班啦.svg";
        img.title = "下班了就该开开心心的玩耍，嘿嘿~";
        img.alt = "下班了就该开开心心的玩耍，嘿嘿~";
      }

      currentTimeHtml = `本站居然运行了 ${dnum} 天<span id='runtime'> ${hnum} 小时 ${mnum} 分 ${snum} 秒 </span><i class='anzhiyufont anzhiyu-icon-heartbeat' style='color:red'></i>`;
    }

    if (document.getElementById("runtimeTextTip")) {
      document.getElementById("runtimeTextTip").innerHTML = currentTimeHtml;
    }
  }

  setInterval(() => {
    updateTime();
    updateHtml();
  }, 1000);
})();</script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>(() => {
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://love-twikoo.xiugou.top/',
      region: '',
      onCommentLoaded: () => {
        anzhiyu.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') setTimeout(runFn,0)
    else getScript('https://cdn.cbd.int/twikoo@1.6.39/dist/twikoo.all.min.js').then(runFn)
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://love-twikoo.xiugou.top/',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(res => {
      countELement.textContent = res[0].count
    }).catch(err => {
      console.error(err)
    })
  }

  const runFn = () => {
    init();
    
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) anzhiyu.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else {
      loadTwikoo()
    }
  } else {
    window.loadOtherComment = loadTwikoo
  }
})()</script><input type="hidden" name="page-type" id="page-type" value="post"></div><div class="app-refresh" id="app-refresh" style="position: fixed;top: -2.2rem;left: 0;right: 0;z-index: 99999;padding: 0 1rem;font-size: 15px;height: 2.2rem;transition: all 0.3s ease;"><div class="app-refresh-wrap" style=" display: flex;color: #fff;height: 100%;align-items: center;justify-content: center;"><label>✨ 有新文章啦！ 👉</label><a href="javascript:void(0)" onclick="location.reload()"><span style="color: #fff;text-decoration: underline;cursor: pointer;">🍗点击食用🍔</span></a></div></div><script>if ('serviceWorker' in navigator) {
if (navigator.serviceWorker.controller) {
navigator.serviceWorker.addEventListener('controllerchange', function() {
showNotification()
})
}
window.addEventListener('load', function() {
navigator.serviceWorker.register('/sw.js')
})
}
function showNotification() {
if (GLOBAL_CONFIG.Snackbar) {
var snackbarBg =
document.documentElement.getAttribute('data-theme') === 'light' ?
GLOBAL_CONFIG.Snackbar.bgLight :
GLOBAL_CONFIG.Snackbar.bgDark
var snackbarPos = GLOBAL_CONFIG.Snackbar.position
Snackbar.show({
text: '✨ 有新文章啦！ 👉',
backgroundColor: snackbarBg,
duration: 500000,
pos: snackbarPos,
actionText: '🍗点击食用🍔',
actionTextColor: '#fff',
onActionClick: function(e) {
location.reload()
},
})
} else {
var showBg =
document.documentElement.getAttribute('data-theme') === 'light' ?
'#3b70fc' :
'#1f1f1f'
var cssText = `top: 0; background: ${showBg};`
document.getElementById('app-refresh').style.cssText = cssText
}
}</script><script>var visitorMail = "";
</script><script async data-pjax src="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/waterfall/waterfall.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/qrcodejs/1.0.0/qrcode.min.js"></script><script src="/js/anzhiyu/right_click_menu.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.9/icon/ali_iconfont_css.css"><script async data-pjax src="/js/anzhiyu.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/aplayer/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.cbd.int/anzhiyu-blog-static@1.0.1/js/APlayer.min.js"></script><script src="https://npm.elemecdn.com/hexo-anzhiyu-music@1.0.1/assets/js/Meting2.min.js"></script><script charset="UTF-8" src="https://cdn.cbd.int/anzhiyu-theme-static@1.1.5/accesskey/accesskey.js"></script></div><div id="popup-window"><div class="popup-window-title">通知</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">你好呀</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div><!-- hexo injector body_end start --><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '1.5s');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '30');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('flink-list-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__flipInY');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('flink-list-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__animated');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('article-sort-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__slideInRight');
    arr[i].setAttribute('data-wow-duration', '1.5s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('site-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__flipInY');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('site-card');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__animated');
    arr[i].setAttribute('data-wow-duration', '3s');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script></div><script defer src="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://cdn.cbd.int/hexo-butterfly-wowjs/lib/wow_init.js"></script><script async src="/js/ali_font.js"></script><!-- hexo injector body_end end --></body></html>